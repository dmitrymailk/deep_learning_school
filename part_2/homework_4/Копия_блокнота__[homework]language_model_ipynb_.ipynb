{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Копия_блокнота__[homework]language_model_ipynb_.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Ot3c4fjZwC4T"},"source":["<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n","<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"P2JdzEXmwRU5"},"source":["---"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8EQ6dFSDXIa9","executionInfo":{"status":"ok","timestamp":1616760445029,"user_tz":-180,"elapsed":604,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"c7ea3871-b171-48b7-ac41-4dd82933b999"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/MyDrive/deep_learning_school/part_2/homework_4/\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/deep_learning_school/part_2/homework_4\n"," best-val-model_89.pt\n"," best-val-model_90_05.pt\n"," best-val-model_full.pt\n","'Копия_блокнота__[homework]language_model_ipynb_.ipynb'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oMohh_6CwC4W"},"source":["### Задача определения частей речи, Part-Of-Speech Tagger (POS)"]},{"cell_type":"markdown","metadata":{"id":"2Aad2tmBwC4Y"},"source":["Мы будем решать задачу определения частей речи (POS-теггинга) с помощью скрытой марковской модели (HMM)."]},{"cell_type":"code","metadata":{"id":"gYYV0mdmwC4f","scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616760643939,"user_tz":-180,"elapsed":630,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"3eae8fe3-e9ec-4428-c40e-2231af06b2fc"},"source":["import nltk\n","import pandas as pd\n","import numpy as np\n","from collections import OrderedDict, deque#\n","from nltk.corpus import brown\n","import matplotlib.pyplot as plt\n","\n","from torchtext.legacy.data import Field, BucketIterator\n","import torchtext\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"FPgI52lRwC4n"},"source":["Вам в помощь http://www.nltk.org/book/"]},{"cell_type":"markdown","metadata":{"id":"hxdJxMEAwC4o"},"source":["Загрузим brown корпус"]},{"cell_type":"code","metadata":{"id":"ZvhXAL_9wC4q","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616760454354,"user_tz":-180,"elapsed":538,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"ab02f3be-f84b-440e-8570-440555c06824"},"source":["nltk.download('brown')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"ASm3Dpggs25b"},"source":["Существует множество наборов грамматических тегов, или тегсетов, например:\n","* НКРЯ\n","* Mystem\n","* UPenn\n","* OpenCorpora (его использует pymorphy2)\n","* Universal Dependencies"]},{"cell_type":"markdown","metadata":{"id":"wto8PSC6wC4v"},"source":["<b>Существует не одна система тегирования, поэтому будьте внимательны, когда прогнозируете тег слов в тексте и вычисляете качество прогноза. Можете получить несправедливо низкое качество вашего решения."]},{"cell_type":"markdown","metadata":{"id":"eJ6tuHA_wC4z"},"source":["На данный момент стандартом является **Universal Dependencies**. Подробнее про проект можно почитать [вот тут](http://universaldependencies.org/), а про теги — [вот тут](http://universaldependencies.org/u/pos/)"]},{"cell_type":"code","metadata":{"id":"Cht7dImWwC42","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616760455844,"user_tz":-180,"elapsed":623,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"3cecca6b-80f9-4725-ce63-49bbe6977550"},"source":["nltk.download('universal_tagset')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"IiTimRRywC47"},"source":["<img src=\"https://4.bp.blogspot.com/-IcFli2wljs0/WrVCw3umY_I/AAAAAAAACYM/UJ_neoUAs3wF95dj2Ouf3BzxXzB_b2TbQCLcBGAs/s1600/postags.png\">\n"]},{"cell_type":"markdown","metadata":{"id":"iyDBMcBSwC48"},"source":["Мы имеем массив предложений пар (слово-тег)"]},{"cell_type":"code","metadata":{"id":"BobflewQwC4-","scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764369480,"user_tz":-180,"elapsed":1010,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"2a04f052-7efc-4b78-f98a-1e46d37b941e"},"source":["brown_tagged_sents = brown.tagged_sents(tagset=\"universal\")\n","brown_tagged_sents"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')], [('The', 'DET'), ('jury', 'NOUN'), ('further', 'ADV'), ('said', 'VERB'), ('in', 'ADP'), ('term-end', 'NOUN'), ('presentments', 'NOUN'), ('that', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('Executive', 'ADJ'), ('Committee', 'NOUN'), (',', '.'), ('which', 'DET'), ('had', 'VERB'), ('over-all', 'ADJ'), ('charge', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('election', 'NOUN'), (',', '.'), ('``', '.'), ('deserves', 'VERB'), ('the', 'DET'), ('praise', 'NOUN'), ('and', 'CONJ'), ('thanks', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('City', 'NOUN'), ('of', 'ADP'), ('Atlanta', 'NOUN'), (\"''\", '.'), ('for', 'ADP'), ('the', 'DET'), ('manner', 'NOUN'), ('in', 'ADP'), ('which', 'DET'), ('the', 'DET'), ('election', 'NOUN'), ('was', 'VERB'), ('conducted', 'VERB'), ('.', '.')], ...]"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FluCYxUjtqY8","executionInfo":{"status":"ok","timestamp":1616764524928,"user_tz":-180,"elapsed":687,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"f7f66119-1a7e-447d-af55-a0a1327615a5"},"source":["len(brown_tagged_sents)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["57340"]},"metadata":{"tags":[]},"execution_count":92}]},{"cell_type":"markdown","metadata":{"id":"jSu1KqRrwC5L"},"source":["Первое предложение"]},{"cell_type":"code","metadata":{"id":"zCHCZPlkwC5N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764526838,"user_tz":-180,"elapsed":615,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"4945089e-8ada-4467-d981-870774b285cf"},"source":["brown_tagged_sents[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('The', 'DET'),\n"," ('Fulton', 'NOUN'),\n"," ('County', 'NOUN'),\n"," ('Grand', 'ADJ'),\n"," ('Jury', 'NOUN'),\n"," ('said', 'VERB'),\n"," ('Friday', 'NOUN'),\n"," ('an', 'DET'),\n"," ('investigation', 'NOUN'),\n"," ('of', 'ADP'),\n"," (\"Atlanta's\", 'NOUN'),\n"," ('recent', 'ADJ'),\n"," ('primary', 'NOUN'),\n"," ('election', 'NOUN'),\n"," ('produced', 'VERB'),\n"," ('``', '.'),\n"," ('no', 'DET'),\n"," ('evidence', 'NOUN'),\n"," (\"''\", '.'),\n"," ('that', 'ADP'),\n"," ('any', 'DET'),\n"," ('irregularities', 'NOUN'),\n"," ('took', 'VERB'),\n"," ('place', 'NOUN'),\n"," ('.', '.')]"]},"metadata":{"tags":[]},"execution_count":93}]},{"cell_type":"markdown","metadata":{"id":"SIV2MiRxwC5Q"},"source":["Все пары (слово-тег)"]},{"cell_type":"code","metadata":{"id":"dVx9e9HcwC5R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764529407,"user_tz":-180,"elapsed":871,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"807d83b3-41b7-4221-f7ac-c62786f8858b"},"source":["brown_tagged_words = brown.tagged_words(tagset='universal')\n","brown_tagged_words"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('The', 'DET'), ('Fulton', 'NOUN'), ...]"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"markdown","metadata":{"id":"y-ADby6LwC5V"},"source":["Проанализируйте данные, с которыми Вы работаете. Используйте `nltk.FreqDist()` для подсчета частоты встречаемости тега и слова в нашем корпусе. Под частой элемента подразумевается кол-во этого элемента в корпусе."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"JzRoXuKFcMZK"},"source":["# Приведем слова к нижнему регистру\n","brown_tagged_words = list(map(lambda x: (x[0].lower(), x[1]), brown_tagged_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4giWaqXjwC5W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764533365,"user_tz":-180,"elapsed":2864,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"3fad5248-ebbf-4ba1-d7f1-b4802d5d4873"},"source":["from collections import Counter\n","\n","print('Кол-во предложений: ', len(brown_tagged_sents))\n","tags = [tag for (word, tag) in brown_tagged_words] # наши теги\n","words = [word for (word, tag) in brown_tagged_words] # наши слова\n","\n","stats_tag_num = Counter()\n","stats_tag_num.update(tags)\n","stats_tag_num = dict(stats_tag_num)\n","\n","stats_word_num = Counter()\n","stats_word_num.update(words)\n","stats_word_num = dict(stats_word_num)\n","\n","\n","\n","tag_num = pd.DataFrame({'tags': list(stats_tag_num.keys()),\n","                     'counts': list(stats_tag_num.values())\n","                     }).sort_values(by=['counts'], ascending=False) # тег - кол-во тега в корпусе\n","word_num = pd.DataFrame({'tags': list(stats_word_num.keys()),\n","                     'counts': list(stats_word_num.values())\n","                     }).sort_values(by=['counts'], ascending=False) # слово - кол-во слова в корпусе"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Кол-во предложений:  57340\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yfiPpCcLwC5Z","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":405},"executionInfo":{"status":"ok","timestamp":1616764535008,"user_tz":-180,"elapsed":596,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"403c507a-1723-40ab-af8e-f36120963389"},"source":["tag_num"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tags</th>\n","      <th>counts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>NOUN</td>\n","      <td>275558</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>VERB</td>\n","      <td>182750</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>.</td>\n","      <td>147565</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ADP</td>\n","      <td>144766</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>DET</td>\n","      <td>137019</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ADJ</td>\n","      <td>83721</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>ADV</td>\n","      <td>56239</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>PRON</td>\n","      <td>49334</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>CONJ</td>\n","      <td>38151</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>PRT</td>\n","      <td>29829</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>NUM</td>\n","      <td>14874</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>X</td>\n","      <td>1386</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    tags  counts\n","1   NOUN  275558\n","3   VERB  182750\n","5      .  147565\n","4    ADP  144766\n","0    DET  137019\n","2    ADJ   83721\n","6    ADV   56239\n","9   PRON   49334\n","7   CONJ   38151\n","8    PRT   29829\n","10   NUM   14874\n","11     X    1386"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"id":"8Y1huw7TwC5b","colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"status":"ok","timestamp":1616764537523,"user_tz":-180,"elapsed":622,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"b5687abd-a381-4299-f6f3-adb578dfcb4d"},"source":["plt.figure(figsize=(12, 5))\n","plt.bar(tag_num['tags'], tag_num['counts'])\n","plt.title(\"Tag_frequency\")\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"gBbhnJsmwC5f","colab":{"base_uri":"https://localhost:8080/","height":196},"executionInfo":{"status":"ok","timestamp":1616764541036,"user_tz":-180,"elapsed":1310,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"c2b8f43f-0031-4d8a-8635-6893ce969448"},"source":["word_num[:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tags</th>\n","      <th>counts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>69971</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>,</td>\n","      <td>58334</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>.</td>\n","      <td>49346</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>of</td>\n","      <td>36412</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>and</td>\n","      <td>28853</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   tags  counts\n","0   the   69971\n","32    ,   58334\n","24    .   49346\n","9    of   36412\n","39  and   28853"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"id":"1WmEOBMkwC5i","colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"status":"ok","timestamp":1616764543857,"user_tz":-180,"elapsed":609,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"7f57122c-2abe-412c-fbc7-cc7d47d4b090"},"source":["plt.figure(figsize=(12, 5))\n","plt.bar(word_num.tags[:10], word_num.counts[:10])\n","plt.title(\"Word_frequency\")\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"n08z2PjMwC5o"},"source":["### Вопрос 1:\n","* Кол-во слова `cat` в корпусе?\n","Ответ = 23"]},{"cell_type":"code","metadata":{"id":"jhB7di3YwC5p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616438807573,"user_tz":-180,"elapsed":641,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"99b917dd-d5ee-4388-b735-b9406ca6e8c5"},"source":["'''your code'''\n","word_num[word_num['tags'] == 'cat']['counts']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["14373    23\n","Name: counts, dtype: int64"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"UsCfVLsewC5s"},"source":["### Вопрос 2:\n","* Самое популярное слово с самым популярным тегом? <br>(*сначала выбираете слова с самым популярным тегом, а затем выбираете самое популярное слово из уже выбранных*)\n","\n","Ответ '<b>to</b>'"]},{"cell_type":"code","metadata":{"id":"oio-XBYkwC5t"},"source":["'''your code'''\n","most_popular_tag = 'NOUN'\n","\n","top_words = set([])\n","for i in range(len(tags)):\n","  tag = tags[i]\n","  if tag == most_popular_tag:\n","    word = words[i]\n","    top_words.update([word])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lc0_jjngSWXu"},"source":["get_word_count = lambda w: int(word_num[word_num['tags'] == w]['counts']) \n","top_words = list(top_words)\n","\n","top_words_counts = [get_word_count(item) for item in top_words]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"id":"ujmiMrMYUiRE","executionInfo":{"status":"ok","timestamp":1616440182313,"user_tz":-180,"elapsed":658,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"6127cf1e-4ad1-4e11-b213-7f0373a8e3b3"},"source":["pd.DataFrame({'words': top_words,\n","                     'counts': top_words_counts\n","                     }).sort_values(by=['counts'], ascending=False)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words</th>\n","      <th>counts</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>27996</th>\n","      <td>to</td>\n","      <td>26158</td>\n","    </tr>\n","    <tr>\n","      <th>12913</th>\n","      <td>a</td>\n","      <td>23195</td>\n","    </tr>\n","    <tr>\n","      <th>11260</th>\n","      <td>in</td>\n","      <td>21337</td>\n","    </tr>\n","    <tr>\n","      <th>22141</th>\n","      <td>for</td>\n","      <td>9489</td>\n","    </tr>\n","    <tr>\n","      <th>28336</th>\n","      <td>i</td>\n","      <td>5164</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14122</th>\n","      <td>arylesterases</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14125</th>\n","      <td>timetables</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14126</th>\n","      <td>bani</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14128</th>\n","      <td>nibelungenlied</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>30245</th>\n","      <td>infidels</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>30246 rows × 2 columns</p>\n","</div>"],"text/plain":["                words  counts\n","27996              to   26158\n","12913               a   23195\n","11260              in   21337\n","22141             for    9489\n","28336               i    5164\n","...               ...     ...\n","14122   arylesterases       1\n","14125      timetables       1\n","14126            bani       1\n","14128  nibelungenlied       1\n","30245        infidels       1\n","\n","[30246 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"K-OGc1rSwC5x"},"source":["Впоследствии обучение моделей может занимать слишком много времени, работайте с подвыборкой, например, только текстами определенных категорий."]},{"cell_type":"markdown","metadata":{"id":"Eb7MhxVRwC5y"},"source":["Категории нашего корпуса:"]},{"cell_type":"code","metadata":{"id":"GSiVcP1TwC51","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616755116131,"user_tz":-180,"elapsed":629,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"678bc98e-0d23-45ec-dc77-88a00dfef148"},"source":["brown.categories()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['adventure',\n"," 'belles_lettres',\n"," 'editorial',\n"," 'fiction',\n"," 'government',\n"," 'hobbies',\n"," 'humor',\n"," 'learned',\n"," 'lore',\n"," 'mystery',\n"," 'news',\n"," 'religion',\n"," 'reviews',\n"," 'romance',\n"," 'science_fiction']"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"MjSlFatJwC53"},"source":["Будем работать с категорией humor"]},{"cell_type":"markdown","metadata":{"id":"_f1rl5x0wC55"},"source":["Cделайте случайное разбиение выборки на обучение и контроль в отношении 9:1. "]},{"cell_type":"code","metadata":{"id":"GX9t-1qowC58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764620030,"user_tz":-180,"elapsed":595,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"4ac01f3b-c856-45e3-ec09-288cc8c61234"},"source":["brown_tagged_sents = brown.tagged_sents(tagset=\"universal\", categories='humor')\n","# Приведем слова к нижнему регистру\n","my_brown_tagged_sents = []\n","for sent in brown_tagged_sents:\n","    my_brown_tagged_sents.append(list(map(lambda x: (x[0].lower(), x[1]), sent)))\n","my_brown_tagged_sents = np.array(my_brown_tagged_sents)\n","\n","from sklearn.model_selection import train_test_split\n","train_sents, test_sents = train_test_split(my_brown_tagged_sents, test_size=0.1, random_state=0,)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"pXkVwUjYwC5-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764622358,"user_tz":-180,"elapsed":761,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"b24e1335-8cbb-430d-af3c-df60dc8234d8"},"source":["len(train_sents)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["947"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"code","metadata":{"id":"JQMjzJ2YwC6C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764622995,"user_tz":-180,"elapsed":789,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"1b12620a-f6d3-4578-b83a-e87b49e6499a"},"source":["len(test_sents)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["106"]},"metadata":{"tags":[]},"execution_count":103}]},{"cell_type":"markdown","metadata":{"id":"_rEasLVcwC6G"},"source":["### Метод максимального правдоподобия для обучения модели\n","\n","* $\\normalsize S = s_0, s_1, ..., s_N$ - скрытые состояния, то есть различные теги\n","* $\\normalsize O = o_0, o_1, ..., o_M$ - различные слова\n","* $\\normalsize a_{i,j} = p(s_j|s_i)$ - вероятность того, что, находясь в скрытом состоянии $s_i$, мы попадем в состояние $s_j$ (элемент матрицы $A$)\n","* $\\normalsize b_{k,j}=p(o_k|s_j)$ - вероятность того, что при скрытом состоянии $s_j$ находится слово $o_k$(элемент матрицы $B$)\n","\n","$$\\normalsize x_t \\in O, y_t \\in S$$\n","$\\normalsize (x_t, y_t)$ - слово и тег, стоящие на месте $t$ $\\Rightarrow$ \n","* $\\normalsize X$ - последовательность слов\n","* $\\normalsize Y$ - последовательность тегов\n","\n","Требуется построить скрытую марковскую модель (class HiddenMarkovModel) и написать метод fit для настройки всех её параметров с помощью оценок максимального правдоподобия по размеченным данным (последовательности пар слово+тег):\n","\n","- Вероятности переходов между скрытыми состояниями $p(y_t | y_{t - 1})$ посчитайте на основе частот биграмм POS-тегов.\n","\n","\n","- Вероятности эмиссий наблюдаемых состояний $p(x_t | y_t)$ посчитайте на основе частот \"POS-тег - слово\".\n","\n","\n","- Распределение вероятностей начальных состояний $p(y_0)$ задайте равномерным.\n","\n","Пример $X = [x_0, x_1], Y = [y_0, y_1]$:<br><br>\n","$$p(X, Y) = p(x_0, x_1, y_0, y_1) = p(y_0) \\cdot p(x_0, x_1, y_1 | y_0) = p(y_0) \\cdot p(x_0 | y_0) \\cdot\n","p(x_1, y_1 | x_0, y_0) = \\\\ = p(y_0) \\cdot p(x_0 | y_0) \\cdot p(y_1 | x_0, y_0) \\cdot p(x_1 | x_0, y_0, y_1)\n","= (\\text{в силу условий нашей модели}) = \\\\ = p(y_0) \\cdot p(x_0 | y_0) \\cdot p(y_1 | y_0) \\cdot p(x_1 | y_1) \\Rightarrow$$ <br>\n","Для последовательности длины $n + 1$:<br>\n","$$p(X, Y) = p(x_0 ... x_{n - 1}, y_0 ... y_{n - 1}) \\cdot p(y_n | y_{n - 1}) \\cdot p(x_n | y_n)$$"]},{"cell_type":"markdown","metadata":{"id":"tysPoe5rwC6I"},"source":["#### Алгоритм Витерби для применения модели\n","\n","\n","Требуется написать метод .predict для определения частей речи на тестовой выборке. Чтобы использовать обученную модель на новых данных, необходимо реализовать алгоритм Витерби. Это алгоритм динамиеского программирования, с помощью которого мы будем находить наиболее вероятную последовательность скрытых состояний модели для фиксированной последовательности слов:\n","\n","$$ \\hat{Y} = \\arg \\max_{Y} p(Y|X) = \\arg \\max_{Y} p(Y, X) $$\n","\n","Пусть $\\normalsize Q_{t,s}$ - самая вероятная последовательность скрытых состояний длины $t$ с окончанием в состоянии $s$. $\\normalsize q_{t, s}$ - вероятность этой последовательности.\n","$$(1)\\: \\normalsize q_{t,s} = \\max_{s'} q_{t - 1, s'} \\cdot p(s | s') \\cdot p(o_t | s)$$\n","$\\normalsize Q_{t,s}$ можно восстановить по argmax-ам."]},{"cell_type":"code","metadata":{"id":"QpEXdhOfwC6J"},"source":["class HiddenMarkovModel:    \n","    def __init__(self):\n","    \n","        pass\n","        \n","    def fit(self, train_tokens_tags_list):\n","        \"\"\"\n","        train_tokens_tags_list: массив предложений пар слово-тег (выборка для train) \n","        \"\"\"\n","        tags = [tag for sent in train_tokens_tags_list\n","                for (word, tag) in sent]\n","        words = [word for sent in train_tokens_tags_list\n","                 for (word, tag) in sent]\n","\n","        stats_tag_num = Counter()\n","        stats_tag_num.update(tags)\n","        # stats_tag_num = dict(stats_tag_num)\n","\n","        stats_word_num = Counter()\n","        stats_word_num.update(words)\n","        # stats_word_num = dict(stats_word_num)\n","        \n","        tag_num = pd.Series(stats_tag_num).sort_index()\n","        word_num = pd.Series(stats_word_num).sort_values(ascending=False)\n","         \n","        self.tags = tag_num.index\n","        self.words = word_num.index\n","        \n","        A = pd.DataFrame({'{}'.format(tag) : [0] * len(tag_num) for tag in tag_num.index}, index=tag_num.index)\n","        B = pd.DataFrame({'{}'.format(tag) : [0] * len(word_num) for tag in tag_num.index}, index=word_num.index)\n","        \n","        # Вычисляем матрицу A и B по частотам слов и тегов\n","        \n","        # sent - предложение\n","        # sent[i][0] - i слово в этом предложении, sent[i][1] - i тег в этом предложении\n","        for sent in train_tokens_tags_list:\n","            for i in range(len(sent)):\n","                # B.loc['''your code'''] += 1 # текущая i-пара слово-тег (обновите матрицу B аналогично A)\n","                B.loc[sent[i][0], sent[i][1]] += 1\n","                if len(sent) - 1 != i: # для последнего тега нет следующего тега\n","                    A.loc[sent[i][1], sent[i + 1][1]] += 1 # пара тег-тег\n","                \n","        \n","        # переходим к вероятностям\n","        \n","        # нормируем по строке, то есть по всем всевозможным следующим тегам\n","        A = A.divide(A.sum(axis=1), axis=0)\n","        \n","        # нормируем по столбцу, то есть по всем всевозможным текущим словам\n","        B = B / np.sum(B, axis=0)\n","        \n","        self.A = A\n","        self.B = B\n","        \n","        return self\n","        \n","    \n","    def predict(self, test_tokens_list):\n","        \"\"\"\n","        test_tokens_list : массив предложений пар слово-тег (выборка для test)\n","        \"\"\"\n","        predict_tags = OrderedDict({i : np.array([]) for i in range(len(test_tokens_list))})\n","        \n","        for i_sent in range(len(test_tokens_list)):\n","            \n","            current_sent = test_tokens_list[i_sent] # текущее предложение\n","            len_sent = len(current_sent) # длина предложения \n","            \n","            q = np.zeros(shape=(len_sent + 1, len(self.tags)))\n","            q[0] = 1 # нулевое состояние (равномерная инициализация по всем s)\n","            back_point = np.zeros(shape=(len_sent + 1, len(self.tags))) # # argmax\n","            \n","            for t in range(len_sent):\n","                \n","                # если мы не встречали такое слово в обучении, то вместо него будет \n","                # самое популярное слово с самым популярным тегом (вопрос 2)\n","                if current_sent[t] not in self.words:\n","                    # current_sent[t] = '''your code'''\n","                    current_sent[t] = 'time'\n","                    \n","                # через max выбираем следующий тег\n","                for i_s in range(len(self.tags)):\n","                    \n","                    s = self.tags[i_s]\n","                    \n","                    # формула (1)\n","                    # q[t + 1][i_s] = np.max(q['''your code'''] *\n","                    #     self.A.loc[:, '''your code'''] * \n","                    #     self.B.loc[current_sent[t], s])\n","                    q[t + 1][i_s] = np.max(q[t,:] *\n","                        self.A.loc[:, s] * \n","                        self.B.loc[current_sent[t], s])\n","                    \n","                    # argmax формула(1)\n","                    \n","                    # argmax, чтобы восстановить последовательность тегов\n","                    # back_point[t + 1][i_s] = (q['''your code'''] * self.A.loc[:, '''your code'''] * \n","                    #     self.B.loc[current_sent[t],s]).reset_index()[s].idxmax() # индекс\n","                    back_point[t + 1][i_s] = (q[t,:] * self.A.loc[:, s] * \n","                        self.B.loc[current_sent[t],s]).reset_index()[s].idxmax() # индекс \n","                    \n","            back_point = back_point.astype('int')\n","            \n","            # выписываем теги, меняя порядок на реальный\n","            back_tag = deque()\n","            current_tag = np.argmax(q[len_sent])\n","            for t in range(len_sent, 0, -1):\n","                back_tag.appendleft(self.tags[current_tag])\n","                current_tag = back_point[t, current_tag]\n","             \n","            predict_tags[i_sent] = np.array(back_tag)\n","        \n","        \n","        return predict_tags                 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0BLgsWkwC6M"},"source":["Обучите скрытую марковскую модель:"]},{"cell_type":"code","metadata":{"id":"ZcSoyUAxwC6M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764644750,"user_tz":-180,"elapsed":8159,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"90130370-adf3-4f2c-b695-37e1601dbbee"},"source":["# my_model = ..,\n","'''your code'''\n","my_model = HiddenMarkovModel()\n","my_model.fit(train_sents)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.HiddenMarkovModel at 0x7f11e18de790>"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"markdown","metadata":{"id":"FeVNt19kwC6P"},"source":["Проверьте работу реализованного алгоритма на следующих модельных примерах, проинтерпретируйте результат.\n","\n","- 'He can stay'\n","- 'a cat and a dog'\n","- 'I have a television'\n","- 'My favourite character'"]},{"cell_type":"code","metadata":{"id":"cMJErf7NwC6Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764645919,"user_tz":-180,"elapsed":1157,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"d85c2d77-c227-4fb0-bbc5-e9e98ec97aad"},"source":["sents = [['He', 'can', 'stay'], ['a', 'cat', 'and', 'a', 'dog'], ['I', 'have', 'a', 'television'],\n","         ['My', 'favourite', 'character']]\n","'''your code'''\n","pred = my_model.predict(sents)\n","pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["OrderedDict([(0, array(['NOUN', 'VERB', 'VERB'], dtype='<U4')),\n","             (1, array(['DET', 'NOUN', 'CONJ', 'DET', 'NOUN'], dtype='<U4')),\n","             (2, array(['NOUN', 'VERB', 'DET', 'NOUN'], dtype='<U4')),\n","             (3, array(['NOUN', 'NOUN', 'NOUN'], dtype='<U4'))])"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"markdown","metadata":{"id":"suDCwbGMwC6T"},"source":["### Вопрос 3:\n","* Какой тег вы получили для слова `can`?\\\n","Ответ = 'VERB'"]},{"cell_type":"code","metadata":{"id":"ReHeG3IjwC6U","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1616447547094,"user_tz":-180,"elapsed":661,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"2b0c76cf-b9cb-4e1e-c889-d24b26befc65"},"source":["'''your code'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'your code'"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"ObAslurlwC6X"},"source":["### Вопрос 4:\n","* Какой тег вы получили для слова `favourite`?\\\n","Ответ = 'NOUN'"]},{"cell_type":"code","metadata":{"id":"94crVrrXwC6Y"},"source":["'''your code'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPC4NZ4HwC6a"},"source":["Примените модель к отложенной выборке Брауновского корпуса и подсчитайте точность определения тегов (accuracy). Сделайте выводы. "]},{"cell_type":"code","metadata":{"id":"-7aioBc1wC6b"},"source":["def accuracy_score(model, sents):\n","    true_pred = 0\n","    num_pred = 0\n","\n","    for sent in sents:\n","        # tags = '''your code'''\n","        # words = '''your code'''\n","        tags = [item[1] for item in sent]\n","        words = [item[0] for item in sent]\n","\n","        pred = model.predict([words])[0]\n","\n","        tags_pred = np.sum([tags == pred])\n","\n","        true_pred += tags_pred\n","        num_pred += len(pred)\n","    print(\"Accuracy:\", true_pred / num_pred * 100, '%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"roesKrPCcMbp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764707243,"user_tz":-180,"elapsed":38636,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"5f12ac73-495e-456d-e534-01dc9d3fafa4"},"source":["accuracy_score(my_model, test_sents)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 88.82847256549678 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ff_W7J8XwC6e"},"source":["### Вопрос 5:\n","* Какое качество вы получили(округлите до одного знака после запятой)?"]},{"cell_type":"code","metadata":{"id":"ptvlpc-6wC6f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616449557507,"user_tz":-180,"elapsed":661,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"2fb9da4f-5b85-4579-b143-2047b1694a95"},"source":["'''your code'''\n","88.8"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["88.8"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"FpAgfZRTwC6h"},"source":["## DefaultTagger"]},{"cell_type":"markdown","metadata":{"id":"9b4cPKyiwC6j"},"source":["### Вопрос 6:\n","* Какое качество вы бы получили, если бы предсказывали любой тег, как самый популярный тег на выборке train(округлите до одного знака после запятой)?"]},{"cell_type":"markdown","metadata":{"id":"Td-0Pe0vwC6k"},"source":["Вы можете испоьзовать DefaultTagger(метод tag для предсказания частей речи предложения)"]},{"cell_type":"code","metadata":{"id":"NfZYlMxJwC6m"},"source":["from nltk.tag import DefaultTagger\n","default_tagger = DefaultTagger(\"NOUN\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOKsSQE26MHk"},"source":["class MyDefaultTagger(DefaultTagger):\n","  def __init__(self, *args):\n","    super().__init__(args)\n","  def predict(self, words):\n","    return [np.array([tag[1][0] for tag in self.tag(item)]) for item in words ]\n","my_default_tagger = MyDefaultTagger(\"NOUN\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CXKibo_cMcB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764707246,"user_tz":-180,"elapsed":17768,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"a7315a92-e9ec-46d0-e4a7-ac11ec9fc5df"},"source":["accuracy_score(my_default_tagger, test_sents)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 20.217498764211566 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lz7Q3BfbwC6o"},"source":["## NLTK, Rnnmorph"]},{"cell_type":"markdown","metadata":{"id":"PZvdB22Oyycz"},"source":["Вспомним первый [семинар](https://colab.research.google.com/drive/1FHZVU6yJT61J8w1hALno0stD4VU36rit?usp=sharing) нашего курса. В том семинаре мы с вами работали c некоторыми библиотеками.\n","\n","Не забудьте преобразовать систему тэгов из `'en-ptb' в 'universal'` с помощью функции `map_tag` или используйте `tagset='universal'`"]},{"cell_type":"code","metadata":{"id":"9bn1TGlGAfuL"},"source":["from nltk.tag.mapping import map_tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJQFfbp8A_cj"},"source":["import nltk\n","nltk.download('averaged_perceptron_tagger')\n","# nltk.pos_tag(..., tagset='universal')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j3YKTT2tC27w"},"source":["!pip install rnnmorph"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LD_61W7N35q"},"source":["from rnnmorph.predictor import RNNMorphPredictor\n","predictor = RNNMorphPredictor(language=\"en\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yzemAv5gD_c8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764745190,"user_tz":-180,"elapsed":4353,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"63b75e9d-704e-4814-bffd-b152ce3af20d"},"source":["predictor.predict(['qwe', 'qwe'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<normal_form=qwe; word=qwe; pos=VERB; tag=Mood=Ind|Tense=Pres|VerbForm=Fin; score=0.5480>,\n"," <normal_form=qwe; word=qwe; pos=NOUN; tag=Number=Sing; score=0.4814>]"]},"metadata":{"tags":[]},"execution_count":116}]},{"cell_type":"code","metadata":{"id":"4iq2vvHMCl-B"},"source":["\n","class MyPredictorModel:\n","  def predict(self, sents):\n","    predicted_tags = []\n","    for sent in sents:\n","      pred_tags = np.array([str(item.pos) for item in predictor.predict(sent)])\n","      predicted_tags.append(pred_tags)\n","\n","    return np.array(predicted_tags)\n","\n","myPredictor = MyPredictorModel()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSZKsJ6SP2Vk","executionInfo":{"status":"ok","timestamp":1616764756879,"user_tz":-180,"elapsed":8508,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"4b567103-61bd-425c-d7f9-50420122d186"},"source":["accuracy_score(myPredictor, test_sents)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 62.827483934750376 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"i1z8x4vvwC6s"},"source":["### Вопрос 7:\n","* Какое качество вы получили, используя каждую из двух библиотек? Сравните их результаты.\n","\n","* Качество с библиотекой rnnmorph должно быть хуже, так как там используется немного другая система тэгов. Какие здесь отличия?"]},{"cell_type":"code","metadata":{"id":"GBd3RgqVwC6s","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1616490898615,"user_tz":-240,"elapsed":917,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"97feefc8-a03d-40cf-9963-ba7f2482411c"},"source":["'''your code'''\n","# Markov Model = 88%\n","# RNN Tagger = 62%"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'your code'"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"5w1W5hSkcMcV"},"source":["## BiLSTMTagger"]},{"cell_type":"markdown","metadata":{"id":"mm1-S3t2cMcW"},"source":["### Подготовка данных"]},{"cell_type":"markdown","metadata":{"id":"GayTl7mUcMcX"},"source":["Изменим структуру данных"]},{"cell_type":"code","metadata":{"id":"CnXcI64fxoj4","scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764920179,"user_tz":-180,"elapsed":701,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"d5152995-6c08-419a-c93c-ff088727fec1"},"source":["pos_data = [list(zip(*sent)) for sent in brown.tagged_sents(tagset=\"universal\", categories='humor')]\n","print(pos_data[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[('It', 'was', 'among', 'these', 'that', 'Hinkle', 'identified', 'a', 'photograph', 'of', 'Barco', '!', '!'), ('PRON', 'VERB', 'ADP', 'DET', 'ADP', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', '.', '.')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DpRE3c-3cMcc"},"source":["До этого мы писали много кода сами, теперь пора эксплуатировать pytorch"]},{"cell_type":"code","metadata":{"id":"gvFlzrYnxokE"},"source":["\n","# наши поля\n","WORD = Field(lower=True)\n","TAG = Field(unk_token=None) # все токены нам извсетны\n","\n","# создаем примеры\n","examples = []\n","for words, tags in pos_data:\n","    examples.append(torchtext.legacy.data.Example.fromlist([list(words), list(tags)], fields=[('words', WORD), ('tags', TAG)]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tjl6u6cpOc1u"},"source":["Вот один наш пример:"]},{"cell_type":"code","metadata":{"id":"dnrzktytN9rL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764925392,"user_tz":-180,"elapsed":646,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"16f72a88-0299-4333-e3b8-f50e945587df"},"source":["print(vars(examples[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'words': ['it', 'was', 'among', 'these', 'that', 'hinkle', 'identified', 'a', 'photograph', 'of', 'barco', '!', '!'], 'tags': ['PRON', 'VERB', 'ADP', 'DET', 'ADP', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', '.', '.']}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nUhTrWCWcMcj"},"source":["Теперь формируем наш датасет"]},{"cell_type":"code","metadata":{"id":"LGKkbZUIxokO","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764927402,"user_tz":-180,"elapsed":733,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"22e2b063-c76e-4bd9-c695-3b2673c5a049"},"source":["# кладем примеры в наш датасет\n","dataset = torchtext.legacy.data.Dataset(examples, fields=[('words', WORD), ('tags', TAG)])\n","\n","train_data, valid_data, test_data = dataset.split(split_ratio=[0.8, 0.1, 0.1])\n","\n","print(f\"Number of training examples: {len(train_data.examples)}\")\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: 842\n","Number of validation examples: 106\n","Number of testing examples: 105\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T89unpppcMcp"},"source":["Построим словари. Параметр `min_freq` выберете сами. При построении словаря испольузем только **train**"]},{"cell_type":"code","metadata":{"id":"tZwkwhlrxoka","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764933361,"user_tz":-180,"elapsed":726,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"60c12b07-be56-4a9f-8f56-3ae26e204860"},"source":["WORD.build_vocab(train_data, min_freq=2)\n","TAG.build_vocab(train_data,)\n","\n","print(f\"Unique tokens in source (ru) vocabulary: {len(WORD.vocab)}\")\n","print(f\"Unique tokens in target (en) vocabulary: {len(TAG.vocab)}\")\n","\n","print(WORD.vocab.itos[::200])\n","print(TAG.vocab.itos)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unique tokens in source (ru) vocabulary: 1298\n","Unique tokens in target (en) vocabulary: 13\n","['<unk>', 'viola', 'care', 'grazie', 'buying', 'knee-length', 'spoke']\n","['<pad>', 'NOUN', 'VERB', '.', 'DET', 'ADP', 'PRON', 'ADJ', 'ADV', 'CONJ', 'PRT', 'NUM', 'X']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vjn07NP-xokl","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616764938480,"user_tz":-180,"elapsed":891,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"698a7397-dc5f-4ede-c2cc-8b8980a7d14e"},"source":["print(vars(train_data.examples[9]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'words': ['however', 'one', 'looks', 'at', 'it', ',', 'therefore', ',', \"i'd\", 'say', 'that', 'your', 'horoscope', 'for', 'this', 'autumn', 'is', 'the', 'reverse', 'of', 'rosy', '.'], 'tags': ['ADV', 'NOUN', 'VERB', 'ADP', 'PRON', '.', 'ADV', '.', 'PRT', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', '.']}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LxgkU4cZcMcz"},"source":["Посмотрим с насколько большими предложениями мы имеем дело"]},{"cell_type":"code","metadata":{"id":"dVpMi1_0xoku","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1616755376304,"user_tz":-180,"elapsed":646,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"df8c7ca5-e97b-4441-af87-4e38fc4a43a7"},"source":["length = map(len, [vars(x)['words'] for x in train_data.examples])\n","\n","plt.figure(figsize=[8, 4])\n","plt.title(\"Length distribution in Train data\")\n","plt.hist(list(length), bins=20);"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAeYAAAEICAYAAACK3Vc9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXsElEQVR4nO3dfZBldX3n8fcnjIjgw/DQEphBZ9QRF60YrV7EwrgoJotgHKpCEVjUUcnOavAxugi6EUwtBhMrPlQMmwkgoyEohSgTNEZEDbFWwMGHyIOuEx57HJhWQBANOvrdP+4ZvbQ90z33gT597/tV1dX3/M65v/Pt06f60+d3zj0nVYUkSWqH31joAiRJ0q8YzJIktYjBLElSixjMkiS1iMEsSVKLGMySJLWIwSwBSVYkqSRLenz/K5N8uWv6R0meNKDa3p7k3EHUOUvfT2hq3W0Q/c3oe2DboA01JLk1yYsG0Ze0MwazFtxC/MEb9jqr6tFVdfMcNRyRZGoefb27qv5oEHXN/Lmr6vam1p8Pov9u89kGs9T3O02Y/ijJA80/IT/q+nrCsGsYhKbupzzc69VoGMh/3ZKGI8mSqtq20HU8XKrqX4FHQ2d0ALgFWDrbNhi3baPx4RGzWivJbyQ5Lcm/J/lBkouT7NPM2z6kuybJ7Um+n+QdXe99VJL1Se5JclOSU7cfnSb5KPAE4B+bo7BTu1Z70mz9zVLbvkk2JLkvybXAk2fM/+URU5Kjk9yY5P4km5O8NclewD8BB3YdDR6Y5MwklyT5+yT3Aa9s2v5+RgmvTvK9JFuSvLVrvRck+d9d0788Kp/t5545NN7UsCHJ3Uk2JfnvXX2d2fwOPtL8LDckmdzJNureBhck+VCSTzfvvSbJk3f03h30N9u2OTTJV5Lc22yLv06y+yBqSPLyJLc1+947Zszb4XqTXNUs9s1mO/9hkr2TXJ5kutknL0+yfFd+fo0Pg1lt9nrgWOC/AAcC9wAfmrHM84CDgSOBdyb5T037GcAK4EnA7wIv2/6Gqno5cDvw+81Q51/Mo7+ZPgT8B3AA8Orma0fOA/5HVT0GeAbwhap6AHgx8L2mhkdX1fea5VcDlwBLgQt30OcLgFXA7wFvyzyG5ef4ubf7GDBFZ3sfB7w7yQu75r+0WWYpsAH467nW2+UE4F3A3sAm4KxdeO92M7fNz4E3A/sBz6Xze/vjfmtIcghwDvByOttiX6A7SHe43qp6frPMM5vt/HE6f2s/DDyRzj9HP2HXtp3GiMGsNnsN8I6qmqqqB4EzgePy0Auf3lVVP6mqbwLfBJ7ZtB8PvLuq7qmqKeCD81znjvr7pXQulPoD4J1V9UBVXQ+s30mfPwMOSfLYpp6vzVHDV6rqU1X1i6r6yU7qfKCqvkXnD/6Jc/5kc0hyEHA48Laq+o+q+gZwLvCKrsW+XFWfac5Jf5RZts9OfLKqrm2Gny8EfruHMh+ybarquqq6uqq2VdWtwN/S+Ueu3xqOAy6vqquafe9PgV9sn7mr662qH1TVJ6rqx1V1P51/CHZWp8aYwaw2eyLwyWa48F7gJjpHKvt3LXNn1+sf05yfpHOUc0fXvO7XO7Oj/rpN0Lk+o7vP23bS5x8ARwO3JfmXJM+do4b51Dpz3QfO4z1zORC4uwmO7r6XdU3P3D57ZP5XiM9n287lIdsmyVObYeE7m+Htd9M5iu23hofsP80Ixw96XW+SPZP8bTM0fh9wFbA0Q7gaXoufwaw2uwN4cVUt7frao6o2z+O9W3jo0ONBM+b381i1aWDbjD53eLVwVX21qlYDjwc+BVw8Rw3zqW3murcPgz8A7Nk17zd3oe/vAfskecyMvuezvR8uM+s/B/g2sKqqHgu8HcgA1rOFrm2cZE86w9m9rvctdE6RPKdZfvtw9yBq1YgxmNUWj0iyR9fXEuD/AGcleSJAkokkq+fZ38XA6c1FN8uA182Yfxed88+7rBnGvRQ4szkSOgRYM9uySXZPclKSx1XVz4D7+NWQ6F3Avkke10MZf9qs++nAq4CPN+3fAI5Osk+S3wTeNON9O/y5q+oO4P8Cf978Dn4LOBmYeeFZmzyGzjb9UZKnAa8dUL+XAC9J8rzmoq4/46F/L+da78zt/Bg655XvTecCxjMGVKdGkMGstvgMnT9c27/OBD5A5wKjzyW5H7gaeM48+/szOhcx3QJ8ns4f2ge75v858L+aYfK3zvL+ubyOzjDoncAFdM7z7sjLgVubIczXACcBVNW3gYuAm5s6dmU4+l/oXLx0JfDeqvpc0/5ROufGbwU+x68Ce7u5fu4T6Vw09z3gk8AZVfX5Xajr4fZW4L8B9wN/x6//vD2pqhuAU4B/oHP0fA+d/Wm+6z0TWN9s5+OB9wOPAr5PZz/+7CDq1GhKVT8jetLikOS1wAlV5QU3klrNI2aNpCQHJDk8nc9CH0znHN8nF7ouSZqLd/7SqNqdzkdYVgL30vns7d8saEWSNA8OZUuS1CIOZUuS1CKtGMreb7/9asWKFQtdhiRJD5vrrrvu+1U1MbO9FcG8YsUKNm7cuNBlSJL0sEky6x0DHcqWJKlFDGZJklpkzmBOcn6SrUmun9H++iTfbp7J+hdd7aen8xzX7yT5r8MoWpKkUTWfc8wX0Hlu6Ee2NyR5AZ3noj6zqh5M8vim/RA6zzt9Op2ns3w+yVObewtLkqQ5zHnEXFVXAXfPaH4tcHbznFKqamvTvhr4WFU9WFW30LmX76EDrFeSpJHW6znmpwK/k+Sa5vmy/7lpX8ZDn5c6xUOf5fpLSdYm2Zhk4/T0dI9lSJI0WnoN5iXAPsBhwP8ELk6yS88Vrap1VTVZVZMTE7/2MS5JksZSr8E8BVxaHdfSeb7sfnQeqN79APfltOsh65IktVqvwfwp4AUASZ5K54EB36fz7NwTkjwyyUpgFXDtIAqVJGkczHlVdpKLgCOA/ZJMAWcA5wPnNx+h+imwpjpPw7ghycXAjcA24JRRuSJ7xWmfHmh/t559zED7kySNhjmDuapO3MGsl+1g+bOAs/opSpKkceWdvyRJahGDWZKkFjGYJUlqEYNZkqQWMZglSWoRg1mSpBYxmCVJahGDWZKkFjGYJUlqEYNZkqQWMZglSWoRg1mSpBYxmCVJahGDWZKkFjGYJUlqEYNZkqQWMZglSWqROYM5yflJtia5fpZ5b0lSSfZrppPkg0k2Jfm3JM8eRtGSJI2q+RwxXwAcNbMxyUHA7wG3dzW/GFjVfK0Fzum/REmSxsecwVxVVwF3zzLrfcCpQHW1rQY+Uh1XA0uTHDCQSiVJGgM9nWNOshrYXFXfnDFrGXBH1/RU0zZbH2uTbEyycXp6upcyJEkaObsczEn2BN4OvLOfFVfVuqqarKrJiYmJfrqSJGlkLOnhPU8GVgLfTAKwHPhakkOBzcBBXcsub9okSdI87PIRc1V9q6oeX1UrqmoFneHqZ1fVncAG4BXN1dmHAT+sqi2DLVmSpNE1n49LXQR8BTg4yVSSk3ey+GeAm4FNwN8BfzyQKiVJGhNzDmVX1YlzzF/R9bqAU/ovS5Kk8eSdvyRJapFeLv5SC6047dMD7/PWs48ZeJ+SpJ3ziFmSpBYxmCVJahGDWZKkFjGYJUlqEYNZkqQWMZglSWoRg1mSpBYxmCVJahGDWZKkFjGYJUlqEYNZkqQWMZglSWoRg1mSpBYxmCVJahGDWZKkFpnzecxJzgdeAmytqmc0bX8J/D7wU+DfgVdV1b3NvNOBk4GfA2+oqn8eUu2L2jCenyxJWvzmc8R8AXDUjLYrgGdU1W8B/w84HSDJIcAJwNOb9/xNkt0GVq0kSSNuzmCuqquAu2e0fa6qtjWTVwPLm9ergY9V1YNVdQuwCTh0gPVKkjTSBnGO+dXAPzWvlwF3dM2batokSdI89BXMSd4BbAMu7OG9a5NsTLJxenq6nzIkSRoZPQdzklfSuSjspKqqpnkzcFDXYsubtl9TVeuqarKqJicmJnotQ5KkkdJTMCc5CjgVeGlV/bhr1gbghCSPTLISWAVc23+ZkiSNh/l8XOoi4AhgvyRTwBl0rsJ+JHBFEoCrq+o1VXVDkouBG+kMcZ9SVT8fVvGSJI2aOYO5qk6cpfm8nSx/FnBWP0VJkjSu5gxmja9B3wTl1rOPGWh/kjSKvCWnJEktYjBLktQiBrMkSS1iMEuS1CIGsyRJLWIwS5LUIgazJEktYjBLktQiBrMkSS1iMEuS1CIGsyRJLWIwS5LUIgazJEktYjBLktQiBrMkSS1iMEuS1CIGsyRJLTJnMCc5P8nWJNd3te2T5Iok322+7920J8kHk2xK8m9Jnj3M4iVJGjXzOWK+ADhqRttpwJVVtQq4spkGeDGwqvlaC5wzmDIlSRoPcwZzVV0F3D2jeTWwvnm9Hji2q/0j1XE1sDTJAYMqVpKkUdfrOeb9q2pL8/pOYP/m9TLgjq7lppq2X5NkbZKNSTZOT0/3WIYkSaOl74u/qqqA6uF966pqsqomJyYm+i1DkqSR0Gsw37V9iLr5vrVp3wwc1LXc8qZNkiTNQ6/BvAFY07xeA1zW1f6K5ursw4Afdg15S5KkOSyZa4EkFwFHAPslmQLOAM4GLk5yMnAbcHyz+GeAo4FNwI+BVw2hZkmSRtacwVxVJ+5g1pGzLFvAKf0WJUnSuPLOX5IktYjBLElSixjMkiS1iMEsSVKLGMySJLWIwSxJUosYzJIktYjBLElSixjMkiS1iMEsSVKLGMySJLWIwSxJUosYzJIktYjBLElSixjMkiS1iMEsSVKLGMySJLVIX8Gc5M1JbkhyfZKLkuyRZGWSa5JsSvLxJLsPqlhJkkZdz8GcZBnwBmCyqp4B7AacALwHeF9VPQW4Bzh5EIVKkjQO+h3KXgI8KskSYE9gC/BC4JJm/nrg2D7XIUnS2Og5mKtqM/Be4HY6gfxD4Drg3qra1iw2BSyb7f1J1ibZmGTj9PR0r2VIkjRS+hnK3htYDawEDgT2Ao6a7/ural1VTVbV5MTERK9lSJI0UvoZyn4RcEtVTVfVz4BLgcOBpc3QNsByYHOfNUqSNDb6CebbgcOS7JkkwJHAjcAXgeOaZdYAl/VXoiRJ46Ofc8zX0LnI62vAt5q+1gFvA/4kySZgX+C8AdQpSdJYWDL3IjtWVWcAZ8xovhk4tJ9+JUkaV975S5KkFjGYJUlqEYNZkqQWMZglSWoRg1mSpBYxmCVJahGDWZKkFjGYJUlqEYNZkqQWMZglSWqRvm7JKe2KFad9eqD93Xr2MQPtT5LawCNmSZJaxGCWJKlFDGZJklrEYJYkqUUMZkmSWsRgliSpRfoK5iRLk1yS5NtJbkry3CT7JLkiyXeb73sPqlhJkkZdv0fMHwA+W1VPA54J3AScBlxZVauAK5tpSZI0Dz0Hc5LHAc8HzgOoqp9W1b3AamB9s9h64Nh+i5QkaVz0c8S8EpgGPpzk60nOTbIXsH9VbWmWuRPYv98iJUkaF/0E8xLg2cA5VfUs4AFmDFtXVQE125uTrE2yMcnG6enpPsqQJGl09BPMU8BUVV3TTF9CJ6jvSnIAQPN962xvrqp1VTVZVZMTExN9lCFJ0ujoOZir6k7gjiQHN01HAjcCG4A1Tdsa4LK+KpQkaYz0+3Sp1wMXJtkduBl4FZ2wvzjJycBtwPF9rkOSpLHRVzBX1TeAyVlmHdlPv9JC8dGUkhaad/6SJKlFDGZJklrEYJYkqUX6vfhLWjCDPh8sSW3gEbMkSS1iMEuS1CIGsyRJLTKS55g99yhJWqw8YpYkqUUMZkmSWsRgliSpRQxmSZJaxGCWJKlFDGZJklrEYJYkqUUMZkmSWsRgliSpRQxmSZJapO9gTrJbkq8nubyZXpnkmiSbknw8ye79lylJ0ngYxBHzG4GbuqbfA7yvqp4C3AOcPIB1SJI0FvoK5iTLgWOAc5vpAC8ELmkWWQ8c2886JEkaJ/0eMb8fOBX4RTO9L3BvVW1rpqeAZbO9McnaJBuTbJyenu6zDEmSRkPPwZzkJcDWqrqul/dX1bqqmqyqyYmJiV7LkCRppPTzPObDgZcmORrYA3gs8AFgaZIlzVHzcmBz/2VKkjQeej5irqrTq2p5Va0ATgC+UFUnAV8EjmsWWwNc1neVkiSNiWF8jvltwJ8k2UTnnPN5Q1iHJEkjqZ+h7F+qqi8BX2pe3wwcOoh+JUkaN975S5KkFjGYJUlqEYNZkqQWMZglSWoRg1mSpBYxmCVJahGDWZKkFjGYJUlqEYNZkqQWMZglSWoRg1mSpBYxmCVJahGDWZKkFjGYJUlqEYNZkqQWMZglSWqRJQtdgDTKVpz26YH2d+vZxwy0P0nt0/MRc5KDknwxyY1JbkjyxqZ9nyRXJPlu833vwZUrSdJo62coexvwlqo6BDgMOCXJIcBpwJVVtQq4spmWJEnz0PNQdlVtAbY0r+9PchOwDFgNHNEsth74EvC2vqqUBDg0Lo2DgVz8lWQF8CzgGmD/JrQB7gT238F71ibZmGTj9PT0IMqQJGnR6zuYkzwa+ATwpqq6r3teVRVQs72vqtZV1WRVTU5MTPRbhiRJI6GvYE7yCDqhfGFVXdo035XkgGb+AcDW/kqUJGl89HNVdoDzgJuq6q+6Zm0A1jSv1wCX9V6eJEnjpZ/PMR8OvBz4VpJvNG1vB84GLk5yMnAbcHx/JUqSND76uSr7y0B2MPvIXvuVJGmceUtOSZJaxGCWJKlFDGZJklrEYJYkqUUMZkmSWsTHPkpjbND33gbvvy31yyNmSZJaxGCWJKlFDGZJklrEYJYkqUUMZkmSWsRgliSpRfy4lKSBGvRHsPz4lcaNwSxprPjZbbWdQ9mSJLWIwSxJUos4lC2p1YYx9Cy1mcEsSX3ygjcN0tCCOclRwAeA3YBzq+rsYa1LkkaJQT/ehhLMSXYDPgT8LjAFfDXJhqq6cRjrkyTt2GI4HTDofx4W8z83w7r461BgU1XdXFU/BT4GrB7SuiRJGhnDGspeBtzRNT0FPKd7gSRrgbXN5I+SfKeP9e0HfL+P92vH3LbD5fYdHrft8Ax82+Y9g+xt8IZU3xNna1ywi7+qah2wbhB9JdlYVZOD6EsP5bYdLrfv8Lhth8dtO1zDGsreDBzUNb28aZMkSTsxrGD+KrAqycokuwMnABuGtC5JkkbGUIayq2pbktcB/0zn41LnV9UNw1hXYyBD4pqV23a43L7D47YdHrftEKWqFroGSZLU8F7ZkiS1iMEsSVKLLPpgTnJUku8k2ZTktIWuZzFLclCSLya5MckNSd7YtO+T5Iok322+773QtS5WSXZL8vUklzfTK5Nc0+y/H28ultQuSrI0ySVJvp3kpiTPdb8dnCRvbv4mXJ/koiR7uO8Oz6IO5q5bf74YOAQ4MckhC1vVorYNeEtVHQIcBpzSbM/TgCurahVwZTOt3rwRuKlr+j3A+6rqKcA9wMkLUtXi9wHgs1X1NOCZdLax++0AJFkGvAGYrKpn0Lmg9wTcd4dmUQcz3vpzoKpqS1V9rXl9P50/bsvobNP1zWLrgWMXpsLFLcly4Bjg3GY6wAuBS5pF3LY9SPI44PnAeQBV9dOquhf320FaAjwqyRJgT2AL7rtDs9iDebZbfy5boFpGSpIVwLOAa4D9q2pLM+tOYP8FKmuxez9wKvCLZnpf4N6q2tZMu//2ZiUwDXy4OU1wbpK9cL8diKraDLwXuJ1OIP8QuA733aFZ7MGsIUjyaOATwJuq6r7uedX5fJ2fsdtFSV4CbK2q6xa6lhG0BHg2cE5VPQt4gBnD1u63vWvOza+m8w/QgcBewFELWtSIW+zB7K0/ByzJI+iE8oVVdWnTfFeSA5r5BwBbF6q+Rexw4KVJbqVzyuWFdM6LLm2GB8H9t1dTwFRVXdNMX0InqN1vB+NFwC1VNV1VPwMupbM/u+8OyWIPZm/9OUDNOc/zgJuq6q+6Zm0A1jSv1wCXPdy1LXZVdXpVLa+qFXT20y9U1UnAF4HjmsXctj2oqjuBO5Ic3DQdCdyI++2g3A4clmTP5m/E9u3rvjski/7OX0mOpnPubvutP89a4JIWrSTPA/4V+Ba/Og/6djrnmS8GngDcBhxfVXcvSJEjIMkRwFur6iVJnkTnCHof4OvAy6rqwYWsbzFK8tt0LqrbHbgZeBWdAw/32wFI8i7gD+l8cuPrwB/ROafsvjsEiz6YJUkaJYt9KFuSpJFiMEuS1CIGsyRJLWIwS5LUIgazJEktYjBLktQiBrMkSS3y/wFsgf7S7wm/uAAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 576x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"yi28N2RBcMc5"},"source":["Для обучения `BiLSTM` лучше использовать colab"]},{"cell_type":"markdown","metadata":{"id":"2DSWm0UjcMc-"},"source":["Для более быстрого и устойчивого обучения сгруппируем наши данные по батчам"]},{"cell_type":"code","metadata":{"id":"PL5JEp4FG-oP"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmwAyhNgxok_"},"source":["\n","BATCH_SIZE = 220\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE, \n","    device = device,\n","    sort_key=_len_sort_key\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6aTjW00nxolI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616765025174,"user_tz":-180,"elapsed":597,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"8362b21d-8724-4ab9-d34f-fff967bf0d6d"},"source":["# посморим  на количество батчей\n","list(map(len, [train_iterator, valid_iterator, test_iterator]))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[4, 1, 1]"]},"metadata":{"tags":[]},"execution_count":132}]},{"cell_type":"markdown","metadata":{"id":"zyLQsizhcMdI"},"source":["### Модель и её обучение"]},{"cell_type":"markdown","metadata":{"id":"-i9oHzcrcMdJ"},"source":["Инициализируем нашу модель"]},{"cell_type":"code","metadata":{"id":"Ff7BLWs_xolS","scrolled":true},"source":["class LSTMTagger(nn.Module):\n","\n","    def __init__(self, input_dim=100, \n","                 emb_dim=100, \n","                 hid_dim=100, \n","                 output_dim=100, \n","                 dropout=0.2, \n","                 bidirectional=False,\n","                 num_layers=1):\n","        super().__init__()\n","\n","        self.embeddings = nn.Embedding(input_dim, emb_dim)\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","        self.rnn = nn.LSTM(input_size=emb_dim, \n","                           hidden_size=hid_dim, \n","                           bidirectional=bidirectional, \n","                           dropout=dropout\n","                           )\n","        # если bidirectional, то предсказываем на основе конкатенации двух hidden\n","        self.tag = nn.Linear((1 + bidirectional) * hid_dim , output_dim)\n","        self.norm = nn.LayerNorm(emb_dim)\n","       \n","    def forward(self, sent):\n","\n","        embedded = self.embeddings(sent)\n","        embedded = self.norm(embedded)\n","        embedded = self.dropout(embedded)\n","\n","        output, (hidden, cell) = self.rnn(embedded)\n","        # print(output.shape, hidden.shape)\n","        # hidden = self.tanh(torch.cat([hidden[-1,:,:], hidden[-2,:,:]], 1))\n","\n","      \n","        prediction = self.tag(\n","            self.dropout(output)\n","        )\n","    \n","        return prediction\n","        \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oSBfvf9HcMd9"},"source":["Погнали обучать"]},{"cell_type":"code","metadata":{"id":"AjD1Y7Rmxolu","scrolled":true},"source":["# бьем нашу выборку на батч, не забывая сначала отсортировать выборку по длине\n","def _len_sort_key(x):\n","    return len(x.words)\n","\n","\n","\n","def train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None):\n","    model.train()\n","    \n","    epoch_loss = 0\n","    history = []\n","    for i, batch in enumerate(iterator):\n","      '''your code'''\n","      # print(batch)\n","      batch_words = batch.words\n","      optimizer.zero_grad()\n","      \n","      output = model(batch_words)\n","      \n","      #tags = [sent len, batch size]\n","      #output = [sent len, batch size, output dim]\n","      \n","      # output = '''your code'''\n","      tags = batch.tags\n","      output = output.view(output.size()[0] * len(batch), 13)\n","      tags = tags.view(-1)\n","      \n","      #tags = [sent len * batch size]\n","      #output = [sent len * batch size, output dim]\n","      # print(output.size(), tags.size())\n","      loss = criterion(output, tags)\n","      \n","      loss.backward()\n","      \n","      # Gradient clipping(решение проблемы взрыва граденты), clip - максимальная норма вектора\n","      # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n","      \n","      optimizer.step()\n","      \n","      epoch_loss += loss.item()\n","      \n","      history.append(loss.cpu().data.numpy())\n","      if (i+1)%10==0 and False:\n","          fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n","\n","          # clear_output(True)\n","          ax[0].plot(history, label='train loss')\n","          ax[0].set_xlabel('Batch')\n","          ax[0].set_title('Train loss')\n","          \n","          if train_history is not None:\n","              ax[1].plot(train_history, label='general train history')\n","              ax[1].set_xlabel('Epoch')\n","          if valid_history is not None:\n","              ax[1].plot(valid_history, label='general valid history')\n","          plt.legend()\n","            \n","          plt.show()\n","\n","        \n","    return epoch_loss / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","  model.eval()\n","  \n","  epoch_loss = 0\n","  \n","  history = []\n","  \n","  with torch.no_grad():\n","  \n","      for i, batch in enumerate(iterator):\n","\n","          '''your code'''\n","\n","          output = model(batch.words)\n","\n","          #tags = [sent len, batch size]\n","          #output = [sent len, batch size, output dim]\n","\n","          # output = '''your code'''\n","          tags = batch.tags\n","          tags = tags.view(-1)\n","\n","          #tags = [sent len * batch size]\n","          #output = [sent len * batch size, output dim]\n","          output = output.view(output.size()[0] * len(batch), 13)\n","\n","          loss = criterion(output, tags)\n","          \n","          epoch_loss += loss.item()\n","      \n","  return epoch_loss / len(iterator)\n","\n","def epoch_time(start_time, end_time):\n","  elapsed_time = end_time - start_time\n","  elapsed_mins = int(elapsed_time / 60)\n","  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","  return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IzTXUajpfrd7","executionInfo":{"status":"ok","timestamp":1616677623764,"user_tz":-180,"elapsed":661,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"1eb79dd1-3629-4716-8025-85dd3cd557a7"},"source":["next(iter(train_iterator))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","[torchtext.legacy.data.batch.Batch of size 32]\n","\t[.words]:[torch.LongTensor of size 57x32]\n","\t[.tags]:[torch.LongTensor of size 57x32]"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Va0R53gbni60","executionInfo":{"status":"ok","timestamp":1616681015591,"user_tz":-180,"elapsed":647,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"f4243f7a-f025-49b1-9961-3d5be2fbb19a"},"source":["torch.ones((66, 32, 13)).view(66*32, 13).size()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2112, 13])"]},"metadata":{"tags":[]},"execution_count":144}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EH9ASxhOqq3G","executionInfo":{"status":"ok","timestamp":1616680585102,"user_tz":-180,"elapsed":637,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"eefd0679-5e51-496a-d5e2-3b1f9a21478e"},"source":["int(next(iter(train_iterator)).words[0].size()[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["32"]},"metadata":{"tags":[]},"execution_count":133}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTKg2mtrkIbo","executionInfo":{"status":"ok","timestamp":1616695635831,"user_tz":-180,"elapsed":806,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"30ebf722-0aa5-4511-f40b-3142a4183ba3"},"source":["print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LSTMTagger(\n","  (embeddings): Embedding(1280, 3)\n","  (dropout): Dropout(p=0, inplace=False)\n","  (rnn): LSTM(3, 4, bidirectional=True)\n","  (tag): Linear(in_features=8, out_features=13, bias=True)\n","  (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EwbnNgN2kRhU","executionInfo":{"status":"ok","timestamp":1616695600133,"user_tz":-180,"elapsed":992,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"c4b443b0-9d30-4c19-e261-d4b06815a4b6"},"source":["print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 4,261 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gZBUJ0bYHDC5"},"source":["import time\n","import math\n","import matplotlib\n","matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from IPython.display import clear_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJdXIyTHxol2","scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616765121464,"user_tz":-180,"elapsed":34409,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"205455c2-b957-4d9d-8484-8aacb5798fcb"},"source":["PAD_IDX = TAG.vocab.stoi['<pad>']\n","criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n","\n","train_history = []\n","valid_history = []\n","\n","N_EPOCHS = 500\n","CLIP = 1.8\n","\n","# параметры модели\n","params = {\n","  'input_dim': len(WORD.vocab.itos),\n","  'output_dim': len(TAG.vocab.itos),\n","  'emb_dim': 11,\n","  'hid_dim': 11,\n","  'dropout': 0.38,\n","  'bidirectional': True,\n","  'num_layers': 2\n","}\n","MODEL_LR = 5e-2\n","\n","model = LSTMTagger(**params).to(device)\n","\n","optimizer = optim.AdamW(model.parameters(),lr=MODEL_LR)\n","\n","# инициализируем веса\n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param, -0.10, 0.10)\n","        \n","model.apply(init_weights)\n","# model.load_state_dict(torch.load('best-val-model_89.pt'))\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, train_history, valid_history)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'best-val-model.pt')\n","\n","    train_history.append(train_loss)\n","    valid_history.append(valid_loss)\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n","print(f\"Best valid loss {best_valid_loss}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.38 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 01 | Time: 0m 0s\n","\tTrain Loss: 2.400 | Train PPL:  11.019\n","\t Val. Loss: 2.225 |  Val. PPL:   9.255\n","Epoch: 02 | Time: 0m 0s\n","\tTrain Loss: 2.185 | Train PPL:   8.887\n","\t Val. Loss: 2.089 |  Val. PPL:   8.077\n","Epoch: 03 | Time: 0m 0s\n","\tTrain Loss: 1.976 | Train PPL:   7.215\n","\t Val. Loss: 1.673 |  Val. PPL:   5.330\n","Epoch: 04 | Time: 0m 0s\n","\tTrain Loss: 1.607 | Train PPL:   4.987\n","\t Val. Loss: 1.297 |  Val. PPL:   3.659\n","Epoch: 05 | Time: 0m 0s\n","\tTrain Loss: 1.306 | Train PPL:   3.693\n","\t Val. Loss: 0.994 |  Val. PPL:   2.702\n","Epoch: 06 | Time: 0m 0s\n","\tTrain Loss: 1.106 | Train PPL:   3.021\n","\t Val. Loss: 0.826 |  Val. PPL:   2.283\n","Epoch: 07 | Time: 0m 0s\n","\tTrain Loss: 0.967 | Train PPL:   2.631\n","\t Val. Loss: 0.697 |  Val. PPL:   2.009\n","Epoch: 08 | Time: 0m 0s\n","\tTrain Loss: 0.856 | Train PPL:   2.353\n","\t Val. Loss: 0.597 |  Val. PPL:   1.816\n","Epoch: 09 | Time: 0m 0s\n","\tTrain Loss: 0.785 | Train PPL:   2.193\n","\t Val. Loss: 0.537 |  Val. PPL:   1.710\n","Epoch: 10 | Time: 0m 0s\n","\tTrain Loss: 0.718 | Train PPL:   2.051\n","\t Val. Loss: 0.482 |  Val. PPL:   1.619\n","Epoch: 11 | Time: 0m 0s\n","\tTrain Loss: 0.668 | Train PPL:   1.950\n","\t Val. Loss: 0.446 |  Val. PPL:   1.561\n","Epoch: 12 | Time: 0m 0s\n","\tTrain Loss: 0.635 | Train PPL:   1.888\n","\t Val. Loss: 0.424 |  Val. PPL:   1.528\n","Epoch: 13 | Time: 0m 0s\n","\tTrain Loss: 0.587 | Train PPL:   1.799\n","\t Val. Loss: 0.407 |  Val. PPL:   1.502\n","Epoch: 14 | Time: 0m 0s\n","\tTrain Loss: 0.568 | Train PPL:   1.765\n","\t Val. Loss: 0.396 |  Val. PPL:   1.487\n","Epoch: 15 | Time: 0m 0s\n","\tTrain Loss: 0.540 | Train PPL:   1.715\n","\t Val. Loss: 0.380 |  Val. PPL:   1.462\n","Epoch: 16 | Time: 0m 0s\n","\tTrain Loss: 0.533 | Train PPL:   1.704\n","\t Val. Loss: 0.370 |  Val. PPL:   1.448\n","Epoch: 17 | Time: 0m 0s\n","\tTrain Loss: 0.521 | Train PPL:   1.684\n","\t Val. Loss: 0.366 |  Val. PPL:   1.441\n","Epoch: 18 | Time: 0m 0s\n","\tTrain Loss: 0.504 | Train PPL:   1.655\n","\t Val. Loss: 0.360 |  Val. PPL:   1.433\n","Epoch: 19 | Time: 0m 0s\n","\tTrain Loss: 0.485 | Train PPL:   1.624\n","\t Val. Loss: 0.357 |  Val. PPL:   1.429\n","Epoch: 20 | Time: 0m 0s\n","\tTrain Loss: 0.478 | Train PPL:   1.613\n","\t Val. Loss: 0.353 |  Val. PPL:   1.424\n","Epoch: 21 | Time: 0m 0s\n","\tTrain Loss: 0.469 | Train PPL:   1.598\n","\t Val. Loss: 0.345 |  Val. PPL:   1.412\n","Epoch: 22 | Time: 0m 0s\n","\tTrain Loss: 0.462 | Train PPL:   1.588\n","\t Val. Loss: 0.340 |  Val. PPL:   1.404\n","Epoch: 23 | Time: 0m 0s\n","\tTrain Loss: 0.455 | Train PPL:   1.577\n","\t Val. Loss: 0.339 |  Val. PPL:   1.403\n","Epoch: 24 | Time: 0m 0s\n","\tTrain Loss: 0.436 | Train PPL:   1.547\n","\t Val. Loss: 0.338 |  Val. PPL:   1.402\n","Epoch: 25 | Time: 0m 0s\n","\tTrain Loss: 0.435 | Train PPL:   1.545\n","\t Val. Loss: 0.334 |  Val. PPL:   1.396\n","Epoch: 26 | Time: 0m 0s\n","\tTrain Loss: 0.439 | Train PPL:   1.551\n","\t Val. Loss: 0.335 |  Val. PPL:   1.398\n","Epoch: 27 | Time: 0m 0s\n","\tTrain Loss: 0.433 | Train PPL:   1.542\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 28 | Time: 0m 0s\n","\tTrain Loss: 0.424 | Train PPL:   1.528\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 29 | Time: 0m 0s\n","\tTrain Loss: 0.420 | Train PPL:   1.522\n","\t Val. Loss: 0.332 |  Val. PPL:   1.394\n","Epoch: 30 | Time: 0m 0s\n","\tTrain Loss: 0.417 | Train PPL:   1.518\n","\t Val. Loss: 0.321 |  Val. PPL:   1.378\n","Epoch: 31 | Time: 0m 0s\n","\tTrain Loss: 0.408 | Train PPL:   1.504\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 32 | Time: 0m 0s\n","\tTrain Loss: 0.412 | Train PPL:   1.509\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 33 | Time: 0m 0s\n","\tTrain Loss: 0.398 | Train PPL:   1.489\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 34 | Time: 0m 0s\n","\tTrain Loss: 0.399 | Train PPL:   1.490\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 35 | Time: 0m 0s\n","\tTrain Loss: 0.394 | Train PPL:   1.483\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 36 | Time: 0m 0s\n","\tTrain Loss: 0.392 | Train PPL:   1.480\n","\t Val. Loss: 0.313 |  Val. PPL:   1.367\n","Epoch: 37 | Time: 0m 0s\n","\tTrain Loss: 0.384 | Train PPL:   1.469\n","\t Val. Loss: 0.312 |  Val. PPL:   1.366\n","Epoch: 38 | Time: 0m 0s\n","\tTrain Loss: 0.398 | Train PPL:   1.489\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 39 | Time: 0m 0s\n","\tTrain Loss: 0.390 | Train PPL:   1.477\n","\t Val. Loss: 0.314 |  Val. PPL:   1.368\n","Epoch: 40 | Time: 0m 0s\n","\tTrain Loss: 0.390 | Train PPL:   1.476\n","\t Val. Loss: 0.313 |  Val. PPL:   1.368\n","Epoch: 41 | Time: 0m 0s\n","\tTrain Loss: 0.386 | Train PPL:   1.471\n","\t Val. Loss: 0.311 |  Val. PPL:   1.365\n","Epoch: 42 | Time: 0m 0s\n","\tTrain Loss: 0.381 | Train PPL:   1.464\n","\t Val. Loss: 0.309 |  Val. PPL:   1.362\n","Epoch: 43 | Time: 0m 0s\n","\tTrain Loss: 0.383 | Train PPL:   1.467\n","\t Val. Loss: 0.312 |  Val. PPL:   1.367\n","Epoch: 44 | Time: 0m 0s\n","\tTrain Loss: 0.382 | Train PPL:   1.465\n","\t Val. Loss: 0.309 |  Val. PPL:   1.363\n","Epoch: 45 | Time: 0m 0s\n","\tTrain Loss: 0.375 | Train PPL:   1.455\n","\t Val. Loss: 0.310 |  Val. PPL:   1.364\n","Epoch: 46 | Time: 0m 0s\n","\tTrain Loss: 0.373 | Train PPL:   1.452\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 47 | Time: 0m 0s\n","\tTrain Loss: 0.373 | Train PPL:   1.452\n","\t Val. Loss: 0.312 |  Val. PPL:   1.366\n","Epoch: 48 | Time: 0m 0s\n","\tTrain Loss: 0.361 | Train PPL:   1.435\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 49 | Time: 0m 0s\n","\tTrain Loss: 0.365 | Train PPL:   1.441\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 50 | Time: 0m 0s\n","\tTrain Loss: 0.368 | Train PPL:   1.445\n","\t Val. Loss: 0.310 |  Val. PPL:   1.363\n","Epoch: 51 | Time: 0m 0s\n","\tTrain Loss: 0.362 | Train PPL:   1.436\n","\t Val. Loss: 0.312 |  Val. PPL:   1.366\n","Epoch: 52 | Time: 0m 0s\n","\tTrain Loss: 0.369 | Train PPL:   1.446\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 53 | Time: 0m 0s\n","\tTrain Loss: 0.357 | Train PPL:   1.430\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 54 | Time: 0m 0s\n","\tTrain Loss: 0.356 | Train PPL:   1.428\n","\t Val. Loss: 0.310 |  Val. PPL:   1.364\n","Epoch: 55 | Time: 0m 0s\n","\tTrain Loss: 0.368 | Train PPL:   1.444\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 56 | Time: 0m 0s\n","\tTrain Loss: 0.362 | Train PPL:   1.437\n","\t Val. Loss: 0.311 |  Val. PPL:   1.364\n","Epoch: 57 | Time: 0m 0s\n","\tTrain Loss: 0.359 | Train PPL:   1.432\n","\t Val. Loss: 0.317 |  Val. PPL:   1.374\n","Epoch: 58 | Time: 0m 0s\n","\tTrain Loss: 0.350 | Train PPL:   1.419\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 59 | Time: 0m 0s\n","\tTrain Loss: 0.357 | Train PPL:   1.428\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 60 | Time: 0m 0s\n","\tTrain Loss: 0.357 | Train PPL:   1.429\n","\t Val. Loss: 0.311 |  Val. PPL:   1.365\n","Epoch: 61 | Time: 0m 0s\n","\tTrain Loss: 0.355 | Train PPL:   1.426\n","\t Val. Loss: 0.311 |  Val. PPL:   1.365\n","Epoch: 62 | Time: 0m 0s\n","\tTrain Loss: 0.352 | Train PPL:   1.422\n","\t Val. Loss: 0.308 |  Val. PPL:   1.361\n","Epoch: 63 | Time: 0m 0s\n","\tTrain Loss: 0.360 | Train PPL:   1.433\n","\t Val. Loss: 0.310 |  Val. PPL:   1.364\n","Epoch: 64 | Time: 0m 0s\n","\tTrain Loss: 0.349 | Train PPL:   1.417\n","\t Val. Loss: 0.310 |  Val. PPL:   1.363\n","Epoch: 65 | Time: 0m 0s\n","\tTrain Loss: 0.346 | Train PPL:   1.413\n","\t Val. Loss: 0.311 |  Val. PPL:   1.364\n","Epoch: 66 | Time: 0m 0s\n","\tTrain Loss: 0.349 | Train PPL:   1.418\n","\t Val. Loss: 0.311 |  Val. PPL:   1.365\n","Epoch: 67 | Time: 0m 0s\n","\tTrain Loss: 0.351 | Train PPL:   1.420\n","\t Val. Loss: 0.312 |  Val. PPL:   1.366\n","Epoch: 68 | Time: 0m 0s\n","\tTrain Loss: 0.352 | Train PPL:   1.422\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 69 | Time: 0m 0s\n","\tTrain Loss: 0.345 | Train PPL:   1.412\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 70 | Time: 0m 0s\n","\tTrain Loss: 0.347 | Train PPL:   1.415\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 71 | Time: 0m 0s\n","\tTrain Loss: 0.345 | Train PPL:   1.412\n","\t Val. Loss: 0.307 |  Val. PPL:   1.359\n","Epoch: 72 | Time: 0m 0s\n","\tTrain Loss: 0.350 | Train PPL:   1.419\n","\t Val. Loss: 0.309 |  Val. PPL:   1.362\n","Epoch: 73 | Time: 0m 0s\n","\tTrain Loss: 0.349 | Train PPL:   1.418\n","\t Val. Loss: 0.309 |  Val. PPL:   1.363\n","Epoch: 74 | Time: 0m 0s\n","\tTrain Loss: 0.343 | Train PPL:   1.410\n","\t Val. Loss: 0.308 |  Val. PPL:   1.361\n","Epoch: 75 | Time: 0m 0s\n","\tTrain Loss: 0.336 | Train PPL:   1.399\n","\t Val. Loss: 0.309 |  Val. PPL:   1.362\n","Epoch: 76 | Time: 0m 0s\n","\tTrain Loss: 0.339 | Train PPL:   1.403\n","\t Val. Loss: 0.314 |  Val. PPL:   1.370\n","Epoch: 77 | Time: 0m 0s\n","\tTrain Loss: 0.342 | Train PPL:   1.408\n","\t Val. Loss: 0.309 |  Val. PPL:   1.362\n","Epoch: 78 | Time: 0m 0s\n","\tTrain Loss: 0.341 | Train PPL:   1.407\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 79 | Time: 0m 0s\n","\tTrain Loss: 0.341 | Train PPL:   1.406\n","\t Val. Loss: 0.309 |  Val. PPL:   1.362\n","Epoch: 80 | Time: 0m 0s\n","\tTrain Loss: 0.343 | Train PPL:   1.409\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 81 | Time: 0m 0s\n","\tTrain Loss: 0.337 | Train PPL:   1.401\n","\t Val. Loss: 0.313 |  Val. PPL:   1.367\n","Epoch: 82 | Time: 0m 0s\n","\tTrain Loss: 0.339 | Train PPL:   1.404\n","\t Val. Loss: 0.319 |  Val. PPL:   1.375\n","Epoch: 83 | Time: 0m 0s\n","\tTrain Loss: 0.337 | Train PPL:   1.401\n","\t Val. Loss: 0.308 |  Val. PPL:   1.361\n","Epoch: 84 | Time: 0m 0s\n","\tTrain Loss: 0.349 | Train PPL:   1.417\n","\t Val. Loss: 0.313 |  Val. PPL:   1.368\n","Epoch: 85 | Time: 0m 0s\n","\tTrain Loss: 0.331 | Train PPL:   1.392\n","\t Val. Loss: 0.308 |  Val. PPL:   1.361\n","Epoch: 86 | Time: 0m 0s\n","\tTrain Loss: 0.332 | Train PPL:   1.394\n","\t Val. Loss: 0.313 |  Val. PPL:   1.368\n","Epoch: 87 | Time: 0m 0s\n","\tTrain Loss: 0.336 | Train PPL:   1.399\n","\t Val. Loss: 0.313 |  Val. PPL:   1.368\n","Epoch: 88 | Time: 0m 0s\n","\tTrain Loss: 0.336 | Train PPL:   1.400\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 89 | Time: 0m 0s\n","\tTrain Loss: 0.331 | Train PPL:   1.392\n","\t Val. Loss: 0.311 |  Val. PPL:   1.364\n","Epoch: 90 | Time: 0m 0s\n","\tTrain Loss: 0.335 | Train PPL:   1.397\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 91 | Time: 0m 0s\n","\tTrain Loss: 0.332 | Train PPL:   1.394\n","\t Val. Loss: 0.315 |  Val. PPL:   1.371\n","Epoch: 92 | Time: 0m 0s\n","\tTrain Loss: 0.336 | Train PPL:   1.400\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 93 | Time: 0m 0s\n","\tTrain Loss: 0.339 | Train PPL:   1.403\n","\t Val. Loss: 0.307 |  Val. PPL:   1.359\n","Epoch: 94 | Time: 0m 0s\n","\tTrain Loss: 0.341 | Train PPL:   1.407\n","\t Val. Loss: 0.310 |  Val. PPL:   1.364\n","Epoch: 95 | Time: 0m 0s\n","\tTrain Loss: 0.335 | Train PPL:   1.398\n","\t Val. Loss: 0.310 |  Val. PPL:   1.364\n","Epoch: 96 | Time: 0m 0s\n","\tTrain Loss: 0.333 | Train PPL:   1.395\n","\t Val. Loss: 0.311 |  Val. PPL:   1.365\n","Epoch: 97 | Time: 0m 0s\n","\tTrain Loss: 0.331 | Train PPL:   1.393\n","\t Val. Loss: 0.314 |  Val. PPL:   1.368\n","Epoch: 98 | Time: 0m 0s\n","\tTrain Loss: 0.330 | Train PPL:   1.391\n","\t Val. Loss: 0.312 |  Val. PPL:   1.366\n","Epoch: 99 | Time: 0m 0s\n","\tTrain Loss: 0.329 | Train PPL:   1.390\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 100 | Time: 0m 0s\n","\tTrain Loss: 0.342 | Train PPL:   1.407\n","\t Val. Loss: 0.308 |  Val. PPL:   1.361\n","Epoch: 101 | Time: 0m 0s\n","\tTrain Loss: 0.330 | Train PPL:   1.391\n","\t Val. Loss: 0.309 |  Val. PPL:   1.361\n","Epoch: 102 | Time: 0m 0s\n","\tTrain Loss: 0.329 | Train PPL:   1.390\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 103 | Time: 0m 0s\n","\tTrain Loss: 0.330 | Train PPL:   1.391\n","\t Val. Loss: 0.313 |  Val. PPL:   1.368\n","Epoch: 104 | Time: 0m 0s\n","\tTrain Loss: 0.325 | Train PPL:   1.385\n","\t Val. Loss: 0.318 |  Val. PPL:   1.375\n","Epoch: 105 | Time: 0m 0s\n","\tTrain Loss: 0.330 | Train PPL:   1.391\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 106 | Time: 0m 0s\n","\tTrain Loss: 0.325 | Train PPL:   1.384\n","\t Val. Loss: 0.313 |  Val. PPL:   1.367\n","Epoch: 107 | Time: 0m 0s\n","\tTrain Loss: 0.330 | Train PPL:   1.392\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 108 | Time: 0m 0s\n","\tTrain Loss: 0.332 | Train PPL:   1.394\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 109 | Time: 0m 0s\n","\tTrain Loss: 0.333 | Train PPL:   1.395\n","\t Val. Loss: 0.312 |  Val. PPL:   1.366\n","Epoch: 110 | Time: 0m 0s\n","\tTrain Loss: 0.331 | Train PPL:   1.392\n","\t Val. Loss: 0.314 |  Val. PPL:   1.368\n","Epoch: 111 | Time: 0m 0s\n","\tTrain Loss: 0.331 | Train PPL:   1.393\n","\t Val. Loss: 0.305 |  Val. PPL:   1.356\n","Epoch: 112 | Time: 0m 0s\n","\tTrain Loss: 0.333 | Train PPL:   1.395\n","\t Val. Loss: 0.309 |  Val. PPL:   1.363\n","Epoch: 113 | Time: 0m 0s\n","\tTrain Loss: 0.324 | Train PPL:   1.382\n","\t Val. Loss: 0.313 |  Val. PPL:   1.368\n","Epoch: 114 | Time: 0m 0s\n","\tTrain Loss: 0.328 | Train PPL:   1.388\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 115 | Time: 0m 0s\n","\tTrain Loss: 0.330 | Train PPL:   1.391\n","\t Val. Loss: 0.313 |  Val. PPL:   1.368\n","Epoch: 116 | Time: 0m 0s\n","\tTrain Loss: 0.325 | Train PPL:   1.384\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 117 | Time: 0m 0s\n","\tTrain Loss: 0.325 | Train PPL:   1.384\n","\t Val. Loss: 0.308 |  Val. PPL:   1.361\n","Epoch: 118 | Time: 0m 0s\n","\tTrain Loss: 0.323 | Train PPL:   1.382\n","\t Val. Loss: 0.313 |  Val. PPL:   1.367\n","Epoch: 119 | Time: 0m 0s\n","\tTrain Loss: 0.321 | Train PPL:   1.379\n","\t Val. Loss: 0.310 |  Val. PPL:   1.363\n","Epoch: 120 | Time: 0m 0s\n","\tTrain Loss: 0.325 | Train PPL:   1.384\n","\t Val. Loss: 0.313 |  Val. PPL:   1.368\n","Epoch: 121 | Time: 0m 0s\n","\tTrain Loss: 0.326 | Train PPL:   1.385\n","\t Val. Loss: 0.313 |  Val. PPL:   1.367\n","Epoch: 122 | Time: 0m 0s\n","\tTrain Loss: 0.327 | Train PPL:   1.387\n","\t Val. Loss: 0.312 |  Val. PPL:   1.367\n","Epoch: 123 | Time: 0m 0s\n","\tTrain Loss: 0.322 | Train PPL:   1.380\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 124 | Time: 0m 0s\n","\tTrain Loss: 0.319 | Train PPL:   1.376\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 125 | Time: 0m 0s\n","\tTrain Loss: 0.322 | Train PPL:   1.380\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 126 | Time: 0m 0s\n","\tTrain Loss: 0.334 | Train PPL:   1.397\n","\t Val. Loss: 0.311 |  Val. PPL:   1.365\n","Epoch: 127 | Time: 0m 0s\n","\tTrain Loss: 0.324 | Train PPL:   1.383\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 128 | Time: 0m 0s\n","\tTrain Loss: 0.325 | Train PPL:   1.384\n","\t Val. Loss: 0.314 |  Val. PPL:   1.370\n","Epoch: 129 | Time: 0m 0s\n","\tTrain Loss: 0.325 | Train PPL:   1.383\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 130 | Time: 0m 0s\n","\tTrain Loss: 0.321 | Train PPL:   1.379\n","\t Val. Loss: 0.313 |  Val. PPL:   1.367\n","Epoch: 131 | Time: 0m 0s\n","\tTrain Loss: 0.326 | Train PPL:   1.385\n","\t Val. Loss: 0.317 |  Val. PPL:   1.372\n","Epoch: 132 | Time: 0m 0s\n","\tTrain Loss: 0.321 | Train PPL:   1.379\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 133 | Time: 0m 0s\n","\tTrain Loss: 0.319 | Train PPL:   1.376\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 134 | Time: 0m 0s\n","\tTrain Loss: 0.319 | Train PPL:   1.376\n","\t Val. Loss: 0.317 |  Val. PPL:   1.372\n","Epoch: 135 | Time: 0m 0s\n","\tTrain Loss: 0.323 | Train PPL:   1.381\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 136 | Time: 0m 0s\n","\tTrain Loss: 0.315 | Train PPL:   1.370\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 137 | Time: 0m 0s\n","\tTrain Loss: 0.324 | Train PPL:   1.383\n","\t Val. Loss: 0.311 |  Val. PPL:   1.365\n","Epoch: 138 | Time: 0m 0s\n","\tTrain Loss: 0.319 | Train PPL:   1.376\n","\t Val. Loss: 0.312 |  Val. PPL:   1.367\n","Epoch: 139 | Time: 0m 0s\n","\tTrain Loss: 0.317 | Train PPL:   1.373\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 140 | Time: 0m 0s\n","\tTrain Loss: 0.320 | Train PPL:   1.378\n","\t Val. Loss: 0.317 |  Val. PPL:   1.372\n","Epoch: 141 | Time: 0m 0s\n","\tTrain Loss: 0.322 | Train PPL:   1.380\n","\t Val. Loss: 0.311 |  Val. PPL:   1.365\n","Epoch: 142 | Time: 0m 0s\n","\tTrain Loss: 0.319 | Train PPL:   1.375\n","\t Val. Loss: 0.312 |  Val. PPL:   1.366\n","Epoch: 143 | Time: 0m 0s\n","\tTrain Loss: 0.322 | Train PPL:   1.380\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 144 | Time: 0m 0s\n","\tTrain Loss: 0.318 | Train PPL:   1.374\n","\t Val. Loss: 0.314 |  Val. PPL:   1.368\n","Epoch: 145 | Time: 0m 0s\n","\tTrain Loss: 0.318 | Train PPL:   1.375\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 146 | Time: 0m 0s\n","\tTrain Loss: 0.320 | Train PPL:   1.377\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 147 | Time: 0m 0s\n","\tTrain Loss: 0.316 | Train PPL:   1.371\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 148 | Time: 0m 0s\n","\tTrain Loss: 0.319 | Train PPL:   1.376\n","\t Val. Loss: 0.314 |  Val. PPL:   1.368\n","Epoch: 149 | Time: 0m 0s\n","\tTrain Loss: 0.323 | Train PPL:   1.382\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 150 | Time: 0m 0s\n","\tTrain Loss: 0.315 | Train PPL:   1.371\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 151 | Time: 0m 0s\n","\tTrain Loss: 0.322 | Train PPL:   1.379\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 152 | Time: 0m 0s\n","\tTrain Loss: 0.313 | Train PPL:   1.367\n","\t Val. Loss: 0.313 |  Val. PPL:   1.368\n","Epoch: 153 | Time: 0m 0s\n","\tTrain Loss: 0.320 | Train PPL:   1.377\n","\t Val. Loss: 0.313 |  Val. PPL:   1.367\n","Epoch: 154 | Time: 0m 0s\n","\tTrain Loss: 0.320 | Train PPL:   1.377\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 155 | Time: 0m 0s\n","\tTrain Loss: 0.324 | Train PPL:   1.382\n","\t Val. Loss: 0.314 |  Val. PPL:   1.370\n","Epoch: 156 | Time: 0m 0s\n","\tTrain Loss: 0.317 | Train PPL:   1.374\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 157 | Time: 0m 0s\n","\tTrain Loss: 0.320 | Train PPL:   1.377\n","\t Val. Loss: 0.324 |  Val. PPL:   1.383\n","Epoch: 158 | Time: 0m 0s\n","\tTrain Loss: 0.319 | Train PPL:   1.376\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 159 | Time: 0m 0s\n","\tTrain Loss: 0.316 | Train PPL:   1.371\n","\t Val. Loss: 0.320 |  Val. PPL:   1.378\n","Epoch: 160 | Time: 0m 0s\n","\tTrain Loss: 0.320 | Train PPL:   1.376\n","\t Val. Loss: 0.311 |  Val. PPL:   1.364\n","Epoch: 161 | Time: 0m 0s\n","\tTrain Loss: 0.315 | Train PPL:   1.370\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 162 | Time: 0m 0s\n","\tTrain Loss: 0.316 | Train PPL:   1.372\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 163 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.364\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 164 | Time: 0m 0s\n","\tTrain Loss: 0.321 | Train PPL:   1.378\n","\t Val. Loss: 0.321 |  Val. PPL:   1.379\n","Epoch: 165 | Time: 0m 0s\n","\tTrain Loss: 0.318 | Train PPL:   1.374\n","\t Val. Loss: 0.309 |  Val. PPL:   1.361\n","Epoch: 166 | Time: 0m 0s\n","\tTrain Loss: 0.313 | Train PPL:   1.368\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 167 | Time: 0m 0s\n","\tTrain Loss: 0.324 | Train PPL:   1.382\n","\t Val. Loss: 0.310 |  Val. PPL:   1.363\n","Epoch: 168 | Time: 0m 0s\n","\tTrain Loss: 0.313 | Train PPL:   1.367\n","\t Val. Loss: 0.318 |  Val. PPL:   1.375\n","Epoch: 169 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 0.311 |  Val. PPL:   1.365\n","Epoch: 170 | Time: 0m 0s\n","\tTrain Loss: 0.323 | Train PPL:   1.381\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 171 | Time: 0m 0s\n","\tTrain Loss: 0.319 | Train PPL:   1.376\n","\t Val. Loss: 0.307 |  Val. PPL:   1.359\n","Epoch: 172 | Time: 0m 0s\n","\tTrain Loss: 0.313 | Train PPL:   1.367\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 173 | Time: 0m 0s\n","\tTrain Loss: 0.316 | Train PPL:   1.371\n","\t Val. Loss: 0.310 |  Val. PPL:   1.363\n","Epoch: 174 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 0.312 |  Val. PPL:   1.367\n","Epoch: 175 | Time: 0m 0s\n","\tTrain Loss: 0.315 | Train PPL:   1.370\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 176 | Time: 0m 0s\n","\tTrain Loss: 0.308 | Train PPL:   1.361\n","\t Val. Loss: 0.313 |  Val. PPL:   1.368\n","Epoch: 177 | Time: 0m 0s\n","\tTrain Loss: 0.321 | Train PPL:   1.379\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 178 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.364\n","\t Val. Loss: 0.315 |  Val. PPL:   1.370\n","Epoch: 179 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 180 | Time: 0m 0s\n","\tTrain Loss: 0.313 | Train PPL:   1.367\n","\t Val. Loss: 0.314 |  Val. PPL:   1.369\n","Epoch: 181 | Time: 0m 0s\n","\tTrain Loss: 0.316 | Train PPL:   1.372\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 182 | Time: 0m 0s\n","\tTrain Loss: 0.314 | Train PPL:   1.369\n","\t Val. Loss: 0.312 |  Val. PPL:   1.365\n","Epoch: 183 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.366\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 184 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.366\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 185 | Time: 0m 0s\n","\tTrain Loss: 0.315 | Train PPL:   1.370\n","\t Val. Loss: 0.312 |  Val. PPL:   1.366\n","Epoch: 186 | Time: 0m 0s\n","\tTrain Loss: 0.316 | Train PPL:   1.372\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 187 | Time: 0m 0s\n","\tTrain Loss: 0.310 | Train PPL:   1.363\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 188 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 189 | Time: 0m 0s\n","\tTrain Loss: 0.313 | Train PPL:   1.368\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 190 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.367\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 191 | Time: 0m 0s\n","\tTrain Loss: 0.317 | Train PPL:   1.372\n","\t Val. Loss: 0.321 |  Val. PPL:   1.379\n","Epoch: 192 | Time: 0m 0s\n","\tTrain Loss: 0.314 | Train PPL:   1.368\n","\t Val. Loss: 0.313 |  Val. PPL:   1.367\n","Epoch: 193 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.365\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 194 | Time: 0m 0s\n","\tTrain Loss: 0.308 | Train PPL:   1.360\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 195 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 196 | Time: 0m 0s\n","\tTrain Loss: 0.310 | Train PPL:   1.363\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 197 | Time: 0m 0s\n","\tTrain Loss: 0.315 | Train PPL:   1.371\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 198 | Time: 0m 0s\n","\tTrain Loss: 0.307 | Train PPL:   1.359\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 199 | Time: 0m 0s\n","\tTrain Loss: 0.310 | Train PPL:   1.363\n","\t Val. Loss: 0.321 |  Val. PPL:   1.379\n","Epoch: 200 | Time: 0m 0s\n","\tTrain Loss: 0.309 | Train PPL:   1.362\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 201 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 202 | Time: 0m 0s\n","\tTrain Loss: 0.308 | Train PPL:   1.360\n","\t Val. Loss: 0.321 |  Val. PPL:   1.378\n","Epoch: 203 | Time: 0m 0s\n","\tTrain Loss: 0.315 | Train PPL:   1.370\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 204 | Time: 0m 0s\n","\tTrain Loss: 0.310 | Train PPL:   1.363\n","\t Val. Loss: 0.325 |  Val. PPL:   1.385\n","Epoch: 205 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.366\n","\t Val. Loss: 0.321 |  Val. PPL:   1.378\n","Epoch: 206 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.366\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 207 | Time: 0m 0s\n","\tTrain Loss: 0.317 | Train PPL:   1.372\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 208 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 0.316 |  Val. PPL:   1.372\n","Epoch: 209 | Time: 0m 0s\n","\tTrain Loss: 0.313 | Train PPL:   1.367\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 210 | Time: 0m 0s\n","\tTrain Loss: 0.313 | Train PPL:   1.368\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 211 | Time: 0m 0s\n","\tTrain Loss: 0.315 | Train PPL:   1.370\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 212 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.354\n","\t Val. Loss: 0.324 |  Val. PPL:   1.382\n","Epoch: 213 | Time: 0m 0s\n","\tTrain Loss: 0.305 | Train PPL:   1.356\n","\t Val. Loss: 0.321 |  Val. PPL:   1.379\n","Epoch: 214 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 215 | Time: 0m 0s\n","\tTrain Loss: 0.304 | Train PPL:   1.355\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 216 | Time: 0m 0s\n","\tTrain Loss: 0.315 | Train PPL:   1.370\n","\t Val. Loss: 0.321 |  Val. PPL:   1.378\n","Epoch: 217 | Time: 0m 0s\n","\tTrain Loss: 0.307 | Train PPL:   1.359\n","\t Val. Loss: 0.323 |  Val. PPL:   1.382\n","Epoch: 218 | Time: 0m 0s\n","\tTrain Loss: 0.308 | Train PPL:   1.360\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 219 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.366\n","\t Val. Loss: 0.323 |  Val. PPL:   1.382\n","Epoch: 220 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 221 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.352\n","\t Val. Loss: 0.323 |  Val. PPL:   1.382\n","Epoch: 222 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.350\n","\t Val. Loss: 0.321 |  Val. PPL:   1.379\n","Epoch: 223 | Time: 0m 0s\n","\tTrain Loss: 0.304 | Train PPL:   1.355\n","\t Val. Loss: 0.329 |  Val. PPL:   1.389\n","Epoch: 224 | Time: 0m 0s\n","\tTrain Loss: 0.307 | Train PPL:   1.359\n","\t Val. Loss: 0.321 |  Val. PPL:   1.379\n","Epoch: 225 | Time: 0m 0s\n","\tTrain Loss: 0.310 | Train PPL:   1.364\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 226 | Time: 0m 0s\n","\tTrain Loss: 0.305 | Train PPL:   1.357\n","\t Val. Loss: 0.328 |  Val. PPL:   1.389\n","Epoch: 227 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.353\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 228 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.354\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 229 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.365\n","\t Val. Loss: 0.326 |  Val. PPL:   1.386\n","Epoch: 230 | Time: 0m 0s\n","\tTrain Loss: 0.305 | Train PPL:   1.357\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 231 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.366\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 232 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.354\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 233 | Time: 0m 0s\n","\tTrain Loss: 0.308 | Train PPL:   1.361\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 234 | Time: 0m 0s\n","\tTrain Loss: 0.304 | Train PPL:   1.355\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 235 | Time: 0m 0s\n","\tTrain Loss: 0.306 | Train PPL:   1.358\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 236 | Time: 0m 0s\n","\tTrain Loss: 0.309 | Train PPL:   1.362\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 237 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.366\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 238 | Time: 0m 0s\n","\tTrain Loss: 0.305 | Train PPL:   1.356\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 239 | Time: 0m 0s\n","\tTrain Loss: 0.307 | Train PPL:   1.360\n","\t Val. Loss: 0.330 |  Val. PPL:   1.392\n","Epoch: 240 | Time: 0m 0s\n","\tTrain Loss: 0.304 | Train PPL:   1.355\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 241 | Time: 0m 0s\n","\tTrain Loss: 0.311 | Train PPL:   1.365\n","\t Val. Loss: 0.324 |  Val. PPL:   1.382\n","Epoch: 242 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.349\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 243 | Time: 0m 0s\n","\tTrain Loss: 0.298 | Train PPL:   1.347\n","\t Val. Loss: 0.324 |  Val. PPL:   1.382\n","Epoch: 244 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.348\n","\t Val. Loss: 0.329 |  Val. PPL:   1.389\n","Epoch: 245 | Time: 0m 0s\n","\tTrain Loss: 0.304 | Train PPL:   1.356\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 246 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.349\n","\t Val. Loss: 0.331 |  Val. PPL:   1.392\n","Epoch: 247 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.327 |  Val. PPL:   1.386\n","Epoch: 248 | Time: 0m 0s\n","\tTrain Loss: 0.304 | Train PPL:   1.355\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 249 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.354\n","\t Val. Loss: 0.337 |  Val. PPL:   1.400\n","Epoch: 250 | Time: 0m 0s\n","\tTrain Loss: 0.305 | Train PPL:   1.357\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 251 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 252 | Time: 0m 0s\n","\tTrain Loss: 0.297 | Train PPL:   1.346\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 253 | Time: 0m 0s\n","\tTrain Loss: 0.302 | Train PPL:   1.353\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 254 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.350\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 255 | Time: 0m 0s\n","\tTrain Loss: 0.307 | Train PPL:   1.360\n","\t Val. Loss: 0.331 |  Val. PPL:   1.392\n","Epoch: 256 | Time: 0m 0s\n","\tTrain Loss: 0.298 | Train PPL:   1.347\n","\t Val. Loss: 0.336 |  Val. PPL:   1.399\n","Epoch: 257 | Time: 0m 0s\n","\tTrain Loss: 0.307 | Train PPL:   1.360\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 258 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.349\n","\t Val. Loss: 0.329 |  Val. PPL:   1.389\n","Epoch: 259 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.352\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 260 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.354\n","\t Val. Loss: 0.321 |  Val. PPL:   1.379\n","Epoch: 261 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.348\n","\t Val. Loss: 0.340 |  Val. PPL:   1.405\n","Epoch: 262 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.344\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 263 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.348\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 264 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.349\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 265 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.349\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 266 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.348\n","\t Val. Loss: 0.324 |  Val. PPL:   1.382\n","Epoch: 267 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.349\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 268 | Time: 0m 0s\n","\tTrain Loss: 0.297 | Train PPL:   1.346\n","\t Val. Loss: 0.323 |  Val. PPL:   1.382\n","Epoch: 269 | Time: 0m 0s\n","\tTrain Loss: 0.302 | Train PPL:   1.353\n","\t Val. Loss: 0.328 |  Val. PPL:   1.387\n","Epoch: 270 | Time: 0m 0s\n","\tTrain Loss: 0.297 | Train PPL:   1.346\n","\t Val. Loss: 0.332 |  Val. PPL:   1.394\n","Epoch: 271 | Time: 0m 0s\n","\tTrain Loss: 0.302 | Train PPL:   1.352\n","\t Val. Loss: 0.330 |  Val. PPL:   1.390\n","Epoch: 272 | Time: 0m 0s\n","\tTrain Loss: 0.312 | Train PPL:   1.366\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 273 | Time: 0m 0s\n","\tTrain Loss: 0.310 | Train PPL:   1.363\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 274 | Time: 0m 0s\n","\tTrain Loss: 0.306 | Train PPL:   1.358\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 275 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.349\n","\t Val. Loss: 0.331 |  Val. PPL:   1.392\n","Epoch: 276 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.354\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 277 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.354\n","\t Val. Loss: 0.317 |  Val. PPL:   1.372\n","Epoch: 278 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.353\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 279 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.344\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 280 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.352\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 281 | Time: 0m 0s\n","\tTrain Loss: 0.297 | Train PPL:   1.345\n","\t Val. Loss: 0.334 |  Val. PPL:   1.397\n","Epoch: 282 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 283 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.349\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 284 | Time: 0m 0s\n","\tTrain Loss: 0.302 | Train PPL:   1.353\n","\t Val. Loss: 0.328 |  Val. PPL:   1.389\n","Epoch: 285 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.340\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 286 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.350\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 287 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.350\n","\t Val. Loss: 0.324 |  Val. PPL:   1.383\n","Epoch: 288 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.350\n","\t Val. Loss: 0.321 |  Val. PPL:   1.378\n","Epoch: 289 | Time: 0m 0s\n","\tTrain Loss: 0.304 | Train PPL:   1.355\n","\t Val. Loss: 0.333 |  Val. PPL:   1.396\n","Epoch: 290 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.350\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 291 | Time: 0m 0s\n","\tTrain Loss: 0.298 | Train PPL:   1.347\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 292 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.351\n","\t Val. Loss: 0.327 |  Val. PPL:   1.386\n","Epoch: 293 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.350\n","\t Val. Loss: 0.326 |  Val. PPL:   1.386\n","Epoch: 294 | Time: 0m 0s\n","\tTrain Loss: 0.298 | Train PPL:   1.348\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 295 | Time: 0m 0s\n","\tTrain Loss: 0.298 | Train PPL:   1.348\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 296 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 297 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.334 |  Val. PPL:   1.396\n","Epoch: 298 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.343\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 299 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 300 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.345\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 301 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.324 |  Val. PPL:   1.382\n","Epoch: 302 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 303 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.343\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 304 | Time: 0m 0s\n","\tTrain Loss: 0.298 | Train PPL:   1.347\n","\t Val. Loss: 0.331 |  Val. PPL:   1.392\n","Epoch: 305 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.341\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 306 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.343\n","\t Val. Loss: 0.333 |  Val. PPL:   1.396\n","Epoch: 307 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.352\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 308 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.343\n","\t Val. Loss: 0.321 |  Val. PPL:   1.378\n","Epoch: 309 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.344\n","\t Val. Loss: 0.331 |  Val. PPL:   1.392\n","Epoch: 310 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.354\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 311 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.344\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 312 | Time: 0m 0s\n","\tTrain Loss: 0.302 | Train PPL:   1.352\n","\t Val. Loss: 0.331 |  Val. PPL:   1.392\n","Epoch: 313 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.329 |  Val. PPL:   1.389\n","Epoch: 314 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.355\n","\t Val. Loss: 0.318 |  Val. PPL:   1.375\n","Epoch: 315 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.326 |  Val. PPL:   1.386\n","Epoch: 316 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 317 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.344\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 318 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.352\n","\t Val. Loss: 0.329 |  Val. PPL:   1.389\n","Epoch: 319 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.345\n","\t Val. Loss: 0.324 |  Val. PPL:   1.383\n","Epoch: 320 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.328 |  Val. PPL:   1.389\n","Epoch: 321 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.341\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 322 | Time: 0m 0s\n","\tTrain Loss: 0.299 | Train PPL:   1.348\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 323 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.316 |  Val. PPL:   1.371\n","Epoch: 324 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.329\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 325 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 326 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.338\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 327 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.319 |  Val. PPL:   1.375\n","Epoch: 328 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.343\n","\t Val. Loss: 0.323 |  Val. PPL:   1.382\n","Epoch: 329 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.343\n","\t Val. Loss: 0.325 |  Val. PPL:   1.385\n","Epoch: 330 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.336\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 331 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.321 |  Val. PPL:   1.378\n","Epoch: 332 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.338 |  Val. PPL:   1.403\n","Epoch: 333 | Time: 0m 0s\n","\tTrain Loss: 0.298 | Train PPL:   1.347\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 334 | Time: 0m 0s\n","\tTrain Loss: 0.303 | Train PPL:   1.354\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 335 | Time: 0m 0s\n","\tTrain Loss: 0.300 | Train PPL:   1.349\n","\t Val. Loss: 0.337 |  Val. PPL:   1.401\n","Epoch: 336 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.343\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 337 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 338 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 339 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.343\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 340 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.345\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 341 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.336\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 342 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.344\n","\t Val. Loss: 0.331 |  Val. PPL:   1.393\n","Epoch: 343 | Time: 0m 0s\n","\tTrain Loss: 0.297 | Train PPL:   1.346\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 344 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.338\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 345 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.331 |  Val. PPL:   1.393\n","Epoch: 346 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.337\n","\t Val. Loss: 0.327 |  Val. PPL:   1.386\n","Epoch: 347 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.337\n","\t Val. Loss: 0.323 |  Val. PPL:   1.382\n","Epoch: 348 | Time: 0m 0s\n","\tTrain Loss: 0.304 | Train PPL:   1.355\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 349 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 350 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.337\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 351 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 352 | Time: 0m 0s\n","\tTrain Loss: 0.297 | Train PPL:   1.345\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 353 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.333\n","\t Val. Loss: 0.321 |  Val. PPL:   1.378\n","Epoch: 354 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 355 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 356 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.337\n","\t Val. Loss: 0.321 |  Val. PPL:   1.378\n","Epoch: 357 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.344\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 358 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.318 |  Val. PPL:   1.374\n","Epoch: 359 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.352\n","\t Val. Loss: 0.326 |  Val. PPL:   1.386\n","Epoch: 360 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.345\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 361 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.323 |  Val. PPL:   1.382\n","Epoch: 362 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.351\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 363 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 364 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 365 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.322 |  Val. PPL:   1.379\n","Epoch: 366 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.324 |  Val. PPL:   1.382\n","Epoch: 367 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.344\n","\t Val. Loss: 0.331 |  Val. PPL:   1.393\n","Epoch: 368 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.334\n","\t Val. Loss: 0.324 |  Val. PPL:   1.383\n","Epoch: 369 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.340\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 370 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.334 |  Val. PPL:   1.396\n","Epoch: 371 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.322 |  Val. PPL:   1.379\n","Epoch: 372 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.340\n","\t Val. Loss: 0.332 |  Val. PPL:   1.394\n","Epoch: 373 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.334\n","\t Val. Loss: 0.331 |  Val. PPL:   1.392\n","Epoch: 374 | Time: 0m 0s\n","\tTrain Loss: 0.297 | Train PPL:   1.346\n","\t Val. Loss: 0.318 |  Val. PPL:   1.375\n","Epoch: 375 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.352\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 376 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.331 |  Val. PPL:   1.393\n","Epoch: 377 | Time: 0m 0s\n","\tTrain Loss: 0.298 | Train PPL:   1.348\n","\t Val. Loss: 0.332 |  Val. PPL:   1.394\n","Epoch: 378 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.334\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 379 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.337\n","\t Val. Loss: 0.334 |  Val. PPL:   1.396\n","Epoch: 380 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 381 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.336\n","\t Val. Loss: 0.324 |  Val. PPL:   1.383\n","Epoch: 382 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.334\n","\t Val. Loss: 0.325 |  Val. PPL:   1.385\n","Epoch: 383 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.336\n","\t Val. Loss: 0.326 |  Val. PPL:   1.386\n","Epoch: 384 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.333\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 385 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.333\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 386 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.335 |  Val. PPL:   1.398\n","Epoch: 387 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.321 |  Val. PPL:   1.379\n","Epoch: 388 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.340\n","\t Val. Loss: 0.333 |  Val. PPL:   1.396\n","Epoch: 389 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.329\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 390 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.340\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 391 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.340\n","\t Val. Loss: 0.331 |  Val. PPL:   1.393\n","Epoch: 392 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.336\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 393 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 394 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 395 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.335\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 396 | Time: 0m 0s\n","\tTrain Loss: 0.301 | Train PPL:   1.352\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 397 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.340\n","\t Val. Loss: 0.336 |  Val. PPL:   1.399\n","Epoch: 398 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.345\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 399 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.335\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 400 | Time: 0m 0s\n","\tTrain Loss: 0.287 | Train PPL:   1.332\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 401 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 402 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.337\n","\t Val. Loss: 0.329 |  Val. PPL:   1.389\n","Epoch: 403 | Time: 0m 0s\n","\tTrain Loss: 0.286 | Train PPL:   1.331\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 404 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.336\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 405 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.329 |  Val. PPL:   1.389\n","Epoch: 406 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.340\n","\t Val. Loss: 0.317 |  Val. PPL:   1.373\n","Epoch: 407 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.334\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 408 | Time: 0m 0s\n","\tTrain Loss: 0.284 | Train PPL:   1.328\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 409 | Time: 0m 0s\n","\tTrain Loss: 0.284 | Train PPL:   1.328\n","\t Val. Loss: 0.331 |  Val. PPL:   1.393\n","Epoch: 410 | Time: 0m 0s\n","\tTrain Loss: 0.286 | Train PPL:   1.331\n","\t Val. Loss: 0.329 |  Val. PPL:   1.389\n","Epoch: 411 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.337\n","\t Val. Loss: 0.328 |  Val. PPL:   1.389\n","Epoch: 412 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.341\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 413 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.340\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 414 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.344\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 415 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.343\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 416 | Time: 0m 0s\n","\tTrain Loss: 0.284 | Train PPL:   1.329\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 417 | Time: 0m 0s\n","\tTrain Loss: 0.284 | Train PPL:   1.328\n","\t Val. Loss: 0.327 |  Val. PPL:   1.386\n","Epoch: 418 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.329\n","\t Val. Loss: 0.330 |  Val. PPL:   1.390\n","Epoch: 419 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.334\n","\t Val. Loss: 0.324 |  Val. PPL:   1.383\n","Epoch: 420 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.336\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 421 | Time: 0m 0s\n","\tTrain Loss: 0.286 | Train PPL:   1.331\n","\t Val. Loss: 0.324 |  Val. PPL:   1.383\n","Epoch: 422 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.334\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 423 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.326 |  Val. PPL:   1.386\n","Epoch: 424 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.344\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 425 | Time: 0m 0s\n","\tTrain Loss: 0.287 | Train PPL:   1.332\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 426 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 427 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.344\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 428 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.342 |  Val. PPL:   1.407\n","Epoch: 429 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.333\n","\t Val. Loss: 0.321 |  Val. PPL:   1.379\n","Epoch: 430 | Time: 0m 0s\n","\tTrain Loss: 0.287 | Train PPL:   1.333\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 431 | Time: 0m 0s\n","\tTrain Loss: 0.287 | Train PPL:   1.332\n","\t Val. Loss: 0.331 |  Val. PPL:   1.392\n","Epoch: 432 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 433 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.337\n","\t Val. Loss: 0.338 |  Val. PPL:   1.403\n","Epoch: 434 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.340\n","\t Val. Loss: 0.337 |  Val. PPL:   1.400\n","Epoch: 435 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 436 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.336 |  Val. PPL:   1.399\n","Epoch: 437 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.341\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 438 | Time: 0m 0s\n","\tTrain Loss: 0.295 | Train PPL:   1.343\n","\t Val. Loss: 0.326 |  Val. PPL:   1.385\n","Epoch: 439 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.331 |  Val. PPL:   1.392\n","Epoch: 440 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.335 |  Val. PPL:   1.397\n","Epoch: 441 | Time: 0m 0s\n","\tTrain Loss: 0.287 | Train PPL:   1.332\n","\t Val. Loss: 0.335 |  Val. PPL:   1.398\n","Epoch: 442 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.332 |  Val. PPL:   1.394\n","Epoch: 443 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.335\n","\t Val. Loss: 0.329 |  Val. PPL:   1.389\n","Epoch: 444 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.332 |  Val. PPL:   1.394\n","Epoch: 445 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.340\n","\t Val. Loss: 0.323 |  Val. PPL:   1.382\n","Epoch: 446 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.337\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 447 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 448 | Time: 0m 0s\n","\tTrain Loss: 0.283 | Train PPL:   1.327\n","\t Val. Loss: 0.339 |  Val. PPL:   1.403\n","Epoch: 449 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.341\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 450 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.329 |  Val. PPL:   1.389\n","Epoch: 451 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.330\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 452 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.340\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 453 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.341\n","\t Val. Loss: 0.334 |  Val. PPL:   1.397\n","Epoch: 454 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.340\n","\t Val. Loss: 0.324 |  Val. PPL:   1.383\n","Epoch: 455 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.337\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 456 | Time: 0m 0s\n","\tTrain Loss: 0.293 | Train PPL:   1.340\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 457 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.324 |  Val. PPL:   1.382\n","Epoch: 458 | Time: 0m 0s\n","\tTrain Loss: 0.286 | Train PPL:   1.332\n","\t Val. Loss: 0.332 |  Val. PPL:   1.393\n","Epoch: 459 | Time: 0m 0s\n","\tTrain Loss: 0.284 | Train PPL:   1.329\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 460 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.337\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 461 | Time: 0m 0s\n","\tTrain Loss: 0.286 | Train PPL:   1.331\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 462 | Time: 0m 0s\n","\tTrain Loss: 0.286 | Train PPL:   1.331\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 463 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.340\n","\t Val. Loss: 0.336 |  Val. PPL:   1.399\n","Epoch: 464 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.334\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 465 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.330 |  Val. PPL:   1.392\n","Epoch: 466 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.334\n","\t Val. Loss: 0.330 |  Val. PPL:   1.390\n","Epoch: 467 | Time: 0m 0s\n","\tTrain Loss: 0.284 | Train PPL:   1.328\n","\t Val. Loss: 0.322 |  Val. PPL:   1.381\n","Epoch: 468 | Time: 0m 0s\n","\tTrain Loss: 0.286 | Train PPL:   1.332\n","\t Val. Loss: 0.338 |  Val. PPL:   1.402\n","Epoch: 469 | Time: 0m 0s\n","\tTrain Loss: 0.286 | Train PPL:   1.332\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 470 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.330\n","\t Val. Loss: 0.326 |  Val. PPL:   1.386\n","Epoch: 471 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.336\n","\t Val. Loss: 0.330 |  Val. PPL:   1.391\n","Epoch: 472 | Time: 0m 0s\n","\tTrain Loss: 0.291 | Train PPL:   1.338\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 473 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.335\n","\t Val. Loss: 0.324 |  Val. PPL:   1.382\n","Epoch: 474 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.344\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 475 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.330\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 476 | Time: 0m 0s\n","\tTrain Loss: 0.296 | Train PPL:   1.345\n","\t Val. Loss: 0.320 |  Val. PPL:   1.377\n","Epoch: 477 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.330\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 478 | Time: 0m 0s\n","\tTrain Loss: 0.279 | Train PPL:   1.322\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 479 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.335\n","\t Val. Loss: 0.319 |  Val. PPL:   1.376\n","Epoch: 480 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.330\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 481 | Time: 0m 0s\n","\tTrain Loss: 0.286 | Train PPL:   1.331\n","\t Val. Loss: 0.322 |  Val. PPL:   1.379\n","Epoch: 482 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.330\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Epoch: 483 | Time: 0m 0s\n","\tTrain Loss: 0.290 | Train PPL:   1.337\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 484 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.330\n","\t Val. Loss: 0.320 |  Val. PPL:   1.378\n","Epoch: 485 | Time: 0m 0s\n","\tTrain Loss: 0.283 | Train PPL:   1.328\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 486 | Time: 0m 0s\n","\tTrain Loss: 0.287 | Train PPL:   1.333\n","\t Val. Loss: 0.333 |  Val. PPL:   1.395\n","Epoch: 487 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.335\n","\t Val. Loss: 0.329 |  Val. PPL:   1.390\n","Epoch: 488 | Time: 0m 0s\n","\tTrain Loss: 0.287 | Train PPL:   1.332\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 489 | Time: 0m 0s\n","\tTrain Loss: 0.289 | Train PPL:   1.335\n","\t Val. Loss: 0.335 |  Val. PPL:   1.397\n","Epoch: 490 | Time: 0m 0s\n","\tTrain Loss: 0.282 | Train PPL:   1.326\n","\t Val. Loss: 0.325 |  Val. PPL:   1.384\n","Epoch: 491 | Time: 0m 0s\n","\tTrain Loss: 0.292 | Train PPL:   1.339\n","\t Val. Loss: 0.319 |  Val. PPL:   1.375\n","Epoch: 492 | Time: 0m 0s\n","\tTrain Loss: 0.288 | Train PPL:   1.333\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 493 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.328 |  Val. PPL:   1.387\n","Epoch: 494 | Time: 0m 0s\n","\tTrain Loss: 0.285 | Train PPL:   1.330\n","\t Val. Loss: 0.324 |  Val. PPL:   1.383\n","Epoch: 495 | Time: 0m 0s\n","\tTrain Loss: 0.287 | Train PPL:   1.333\n","\t Val. Loss: 0.332 |  Val. PPL:   1.394\n","Epoch: 496 | Time: 0m 0s\n","\tTrain Loss: 0.287 | Train PPL:   1.332\n","\t Val. Loss: 0.325 |  Val. PPL:   1.383\n","Epoch: 497 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.328 |  Val. PPL:   1.388\n","Epoch: 498 | Time: 0m 0s\n","\tTrain Loss: 0.297 | Train PPL:   1.346\n","\t Val. Loss: 0.322 |  Val. PPL:   1.380\n","Epoch: 499 | Time: 0m 0s\n","\tTrain Loss: 0.287 | Train PPL:   1.332\n","\t Val. Loss: 0.327 |  Val. PPL:   1.387\n","Epoch: 500 | Time: 0m 0s\n","\tTrain Loss: 0.294 | Train PPL:   1.342\n","\t Val. Loss: 0.323 |  Val. PPL:   1.381\n","Best valid loss 0.30484575033187866\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fr860UPacMeI"},"source":["### Применение модели"]},{"cell_type":"code","metadata":{"id":"5sDAfAq9xol9"},"source":["def accuracy_model(model, iterator):\n","  model.eval()\n","    \n","  true_pred = 0\n","  num_pred = 0\n","    \n","  with torch.no_grad():\n","    for i, batch in enumerate(iterator):\n","      '''your code'''\n","      output = model(batch.words)\n","      true_tags = batch.tags\n","      \n","      output = F.log_softmax(output, dim=2)\n","      output = torch.argmax(output, dim=2)\n","      \n","      output = output.view(-1)\n","      true_tags = true_tags.view(-1)\n","      predict_tags = output\n","\n","      true_pred = torch.sum((true_tags == predict_tags) & (true_tags != PAD_IDX))\n","      num_pred = torch.prod(torch.tensor(true_tags.shape)) - (true_tags == PAD_IDX).sum()\n","\n","      true_pred, num_pred = int(true_pred), int(num_pred)\n","      true_pred += true_pred\n","      num_pred += num_pred\n","    \n","  return round(true_pred / num_pred * 100, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2n0H85mxomE","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616765126212,"user_tz":-180,"elapsed":660,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"b671e07c-b884-4f72-fe9f-c8c02b4c85d9"},"source":["best_model = LSTMTagger(**params).to(device)\n","best_model.load_state_dict(torch.load('best-val-model.pt'))\n","\n","print(\"Accuracy:\", accuracy_model(best_model, test_iterator), '%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 90.257 %\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.38 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"EJLqq8IHcMdQ"},"source":["Подсчитаем количество обучаемых параметров нашей модели"]},{"cell_type":"code","metadata":{"id":"_Auu53Kdxolm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616765192195,"user_tz":-180,"elapsed":798,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"454cbcfd-2d10-4546-be5b-9dd21b1dd594"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(best_model):,} trainable parameters')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The model has 16,711 trainable parameters\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FacTKSPJcMeP"},"source":["Вы можете улучшить качество, изменяя параметры модели. Но чтобы добиться нужного качества, вам неообходимо взять все выборку, а не только категорию `humor`."]},{"cell_type":"code","metadata":{"id":"QXqXg0gbcMeR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616765217574,"user_tz":-180,"elapsed":5534,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"deeff1e1-9a1a-43aa-d5e8-b44412cdabdc"},"source":["brown_tagged_sents_full = brown.tagged_sents(tagset=\"universal\")\n","pos_data_full = [list(zip(*sent)) for sent in brown_tagged_sents_full]\n","\n","WORD_FULL = Field(lower=True)\n","TAG_FULL = Field(unk_token=None)\n","\n","examples_full = []\n","for words, tags in pos_data_full:\n","    examples_full.append(torchtext.legacy.data.Example.fromlist([list(words), list(tags)], \n","                                                    fields=[('words', WORD_FULL), ('tags', TAG_FULL)]))\n","    \n","dataset_full = torchtext.legacy.data.Dataset(examples_full, fields=[('words', WORD_FULL), ('tags', TAG_FULL)])\n","train_data_full, valid_data_full, test_data_full = dataset_full.split(split_ratio=[0.8, 0.1, 0.1])\n","\n","\n","WORD_FULL.build_vocab(train_data_full, min_freq=10)\n","TAG_FULL.build_vocab(train_data_full,)\n","\n","print(f\"Unique tokens in source (ru) vocabulary: {len(WORD_FULL.vocab)}\")\n","print(f\"Unique tokens in target (en) vocabulary: {len(TAG_FULL.vocab)}\")\n","\n","print(WORD_FULL.vocab.itos[::200])\n","print(TAG_FULL.vocab.itos)\n","BATCH_SIZE = 128\n","train_iterator_full, valid_iterator_full, test_iterator_full = BucketIterator.splits(\n","    (train_data_full, valid_data_full, test_data_full), \n","    batch_size = BATCH_SIZE, \n","    device = 'cuda',\n","    sort_key=_len_sort_key\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Unique tokens in source (ru) vocabulary: 7286\n","Unique tokens in target (en) vocabulary: 13\n","['<unk>', 'war', 'love', 'island', 'theory', '15', 'carefully', 'personnel', 'politics', '11', 'introduced', 'pushed', 'author', 'description', 'patients', 'cleaning', 'constructed', 'albert', 'radical', 'frightened', 'pack', 'undoubtedly', 'anglo-saxon', 'anticipation', 'worn', 'solely', 'lesser', 'composite', 'straightened', 'humble', 'a.l.a.m.', 'neighbor', 'bloom', 'joints', 'vince', 'forthcoming', 'short-term']\n","['<pad>', 'NOUN', 'VERB', '.', 'ADP', 'DET', 'ADJ', 'ADV', 'PRON', 'CONJ', 'PRT', 'NUM', 'X']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0BUicQpESdn","executionInfo":{"status":"ok","timestamp":1616765222078,"user_tz":-180,"elapsed":606,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"38fbd78c-bb90-4619-c386-8148dc3f627c"},"source":["list(map(len, [train_iterator_full, valid_iterator_full, test_iterator_full]))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[359, 45, 45]"]},"metadata":{"tags":[]},"execution_count":143}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wr7Xjk9z4Vim","executionInfo":{"status":"ok","timestamp":1616765308237,"user_tz":-180,"elapsed":84854,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"218d8fbf-9599-4af9-c9d0-198d8ddacf1f"},"source":["train_history = []\n","valid_history = []\n","\n","N_EPOCHS = 20\n","CLIP = 1.8\n","\n","PAD_IDX = TAG_FULL.vocab.stoi['<pad>']\n","criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n","\n","# параметры модели\n","params = {\n","  'input_dim': len(WORD_FULL.vocab.itos),\n","  'output_dim': len(TAG_FULL.vocab.itos),\n","  'emb_dim': 11,\n","  'hid_dim': 11,\n","  'dropout': 0.03,\n","  'bidirectional': True,\n","  'num_layers': 3\n","}\n","MODEL_LR = 1e-3\n","\n","\n","\n","# params = {\n","# 'input_dim': len(WORD_FULL.vocab),\n","# 'output_dim': len(TAG_FULL.vocab),\n","# 'emb_dim': 200,\n","# 'hid_dim': 512,\n","# 'dropout': 0.4,\n","# 'bidirectional': True,\n","# 'num_layers': 2\n","# }\n","# MODEL_LR = 1e-5\n","\n","model_full = LSTMTagger(**params).to(device)\n","\n","\n","optimizer_full = optim.AdamW(model_full.parameters(),lr=MODEL_LR)\n","\n","# инициализируем веса\n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param, -0.08, 0.08)\n","        \n","model_full.apply(init_weights)\n","model_full.load_state_dict(torch.load('best-val-model_full_95_34_small.pt'))\n","\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model_full, train_iterator_full, optimizer_full, criterion, CLIP, train_history, valid_history)\n","    valid_loss = evaluate(model_full, valid_iterator_full, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model_full.state_dict(), 'best-val-model_full.pt')\n","\n","    train_history.append(train_loss)\n","    valid_history.append(valid_loss)\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n","print(f\"Best valid loss {best_valid_loss}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.03 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 01 | Time: 0m 4s\n","\tTrain Loss: 0.116 | Train PPL:   1.123\n","\t Val. Loss: 0.132 |  Val. PPL:   1.142\n","Epoch: 02 | Time: 0m 4s\n","\tTrain Loss: 0.113 | Train PPL:   1.119\n","\t Val. Loss: 0.131 |  Val. PPL:   1.140\n","Epoch: 03 | Time: 0m 4s\n","\tTrain Loss: 0.111 | Train PPL:   1.118\n","\t Val. Loss: 0.131 |  Val. PPL:   1.139\n","Epoch: 04 | Time: 0m 4s\n","\tTrain Loss: 0.110 | Train PPL:   1.116\n","\t Val. Loss: 0.130 |  Val. PPL:   1.139\n","Epoch: 05 | Time: 0m 4s\n","\tTrain Loss: 0.109 | Train PPL:   1.115\n","\t Val. Loss: 0.130 |  Val. PPL:   1.139\n","Epoch: 06 | Time: 0m 4s\n","\tTrain Loss: 0.108 | Train PPL:   1.114\n","\t Val. Loss: 0.130 |  Val. PPL:   1.139\n","Epoch: 07 | Time: 0m 4s\n","\tTrain Loss: 0.107 | Train PPL:   1.113\n","\t Val. Loss: 0.129 |  Val. PPL:   1.138\n","Epoch: 08 | Time: 0m 4s\n","\tTrain Loss: 0.107 | Train PPL:   1.112\n","\t Val. Loss: 0.129 |  Val. PPL:   1.138\n","Epoch: 09 | Time: 0m 4s\n","\tTrain Loss: 0.106 | Train PPL:   1.112\n","\t Val. Loss: 0.129 |  Val. PPL:   1.138\n","Epoch: 10 | Time: 0m 4s\n","\tTrain Loss: 0.105 | Train PPL:   1.111\n","\t Val. Loss: 0.129 |  Val. PPL:   1.138\n","Epoch: 11 | Time: 0m 4s\n","\tTrain Loss: 0.105 | Train PPL:   1.110\n","\t Val. Loss: 0.129 |  Val. PPL:   1.138\n","Epoch: 12 | Time: 0m 4s\n","\tTrain Loss: 0.104 | Train PPL:   1.110\n","\t Val. Loss: 0.129 |  Val. PPL:   1.138\n","Epoch: 13 | Time: 0m 4s\n","\tTrain Loss: 0.104 | Train PPL:   1.109\n","\t Val. Loss: 0.129 |  Val. PPL:   1.138\n","Epoch: 14 | Time: 0m 4s\n","\tTrain Loss: 0.103 | Train PPL:   1.109\n","\t Val. Loss: 0.129 |  Val. PPL:   1.138\n","Epoch: 15 | Time: 0m 4s\n","\tTrain Loss: 0.103 | Train PPL:   1.108\n","\t Val. Loss: 0.130 |  Val. PPL:   1.139\n","Epoch: 16 | Time: 0m 4s\n","\tTrain Loss: 0.102 | Train PPL:   1.108\n","\t Val. Loss: 0.129 |  Val. PPL:   1.138\n","Epoch: 17 | Time: 0m 4s\n","\tTrain Loss: 0.102 | Train PPL:   1.107\n","\t Val. Loss: 0.130 |  Val. PPL:   1.138\n","Epoch: 18 | Time: 0m 4s\n","\tTrain Loss: 0.102 | Train PPL:   1.107\n","\t Val. Loss: 0.130 |  Val. PPL:   1.138\n","Epoch: 19 | Time: 0m 4s\n","\tTrain Loss: 0.101 | Train PPL:   1.107\n","\t Val. Loss: 0.129 |  Val. PPL:   1.138\n","Epoch: 20 | Time: 0m 4s\n","\tTrain Loss: 0.101 | Train PPL:   1.106\n","\t Val. Loss: 0.130 |  Val. PPL:   1.138\n","Best valid loss 0.12907574160231483\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gnpi2R6rcMeU"},"source":["Вам неоходимо добиться качества не меньше, чем `accuracy = 93 %` "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"948xHH4j-OPy","executionInfo":{"status":"ok","timestamp":1616765311708,"user_tz":-180,"elapsed":1228,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"910e4e40-64ec-4216-dd1f-69806184df18"},"source":["best_model_full = LSTMTagger(**params).to(device)\n","best_model_full.load_state_dict(torch.load('best-val-model_full.pt'))\n","\n","# best_model = LSTMTagger(**params).to(device)\n","# best_model.load_state_dict(torch.load('best-val-model.pt'))\n","# print(\"Accuracy:\", accuracy_model(model, test_iterator_full), '%')\n","print(\"Accuracy:\", accuracy_model(best_model_full, test_iterator_full), '%')\n","# print(\"Accuracy:\", accuracy_model(best_model, test_iterator_full), '%')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 95.657 %\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.03 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"TqD1lZuwxomK","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616765328928,"user_tz":-180,"elapsed":1302,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"f9a23c91-a5c5-4a40-ff55-e7a2b953cb22"},"source":["best_model_full = LSTMTagger(**params).to(device)\n","best_model_full.load_state_dict(torch.load('best-val-model_full.pt'))\n","acc = accuracy_model(best_model_full, test_iterator_full) \n","print(acc)\n","assert acc >= 93"],"execution_count":null,"outputs":[{"output_type":"stream","text":["95.657\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.03 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"nVfdJM-lcMeZ"},"source":["Пример решение нашей задачи:"]},{"cell_type":"code","metadata":{"id":"W3GUbwldxomW"},"source":["def print_tags(model, data):\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        words, _ = data\n","        example = torch.LongTensor([WORD_FULL.vocab.stoi[elem] for elem in words]).unsqueeze(1).to(device)\n","        \n","        output = model(example).argmax(dim=-1).cpu().numpy()\n","        tags = [TAG_FULL.vocab.itos[int(elem)] for elem in output]\n","\n","        for token, tag in zip(words, tags):\n","            print(f'{token:15s}{tag}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"4mQoHc_EcMed","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616765335421,"user_tz":-180,"elapsed":955,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"07ffbbfb-d0c9-412f-d178-7e32da407e00"},"source":["print_tags(best_model_full, pos_data[-1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["From           VERB\n","what           DET\n","I              NOUN\n","was            VERB\n","able           ADJ\n","to             PRT\n","gauge          VERB\n","in             ADP\n","a              DET\n","swift          NOUN\n",",              .\n","greedy         ADJ\n","glance         NOUN\n",",              .\n","the            DET\n","figure         NOUN\n","inside         ADP\n","the            DET\n","coral-colored  NOUN\n","boucle         NOUN\n","dress          NOUN\n","was            VERB\n","stupefying     ADJ\n",".              .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"zMIJDOBmwC6v"},"source":["\n","## Сравните результаты моделей HiddenMarkov, LstmTagger:\n","* при обучение на маленькой части корпуса, например, на категории humor\n","* при обучении на всем корпусе"]},{"cell_type":"code","metadata":{"id":"uDdsG2AjO-sp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616767378555,"user_tz":-180,"elapsed":391275,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"5820b03f-23cc-43ad-bf7f-5a7ece88b435"},"source":["brown_tagged_sents = brown.tagged_sents(tagset=\"universal\")\n","# Приведем слова к нижнему регистру\n","my_brown_tagged_sents = []\n","for sent in brown.tagged_sents(tagset=\"universal\"):\n","    my_brown_tagged_sents.append(list(map(lambda x: (x[0].lower(), x[1]), sent)))\n","my_brown_tagged_sents = np.array(my_brown_tagged_sents)\n","\n","from sklearn.model_selection import train_test_split\n","train_sents_full, test_sents_full = train_test_split(my_brown_tagged_sents, test_size=0.1, random_state=0,)\n","\n","my_model_markov_full = HiddenMarkovModel()\n","my_model_markov_full.fit(train_sents_full)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  \n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<__main__.HiddenMarkovModel at 0x7f114daeb450>"]},"metadata":{"tags":[]},"execution_count":151}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UimhNDQkw419","executionInfo":{"status":"ok","timestamp":1616769507797,"user_tz":-180,"elapsed":2511768,"user":{"displayName":"dim web","photoUrl":"","userId":"03939316973290678021"}},"outputId":"387841ec-c508-4f04-99b3-7cc022a0222b"},"source":["accuracy_score(my_model_markov_full, test_sents_full)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 96.26295331104619 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ipg99RBWFOgn"},"source":["Ну как видим на маленьких объемах справилась лучше LSTM, а на больших марковская модель. Думаю можно из LSTM выжать больше но мне уже надоело."]}]}